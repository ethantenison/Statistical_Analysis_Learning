s <- my.s.resid
the.replication <- function(reg,s,x_Np1=0){
# Make bootstrap residuals
ep.star <- sample(s,size=length(reg$residuals),replace=TRUE)
# Make bootstrap Y
y.star <- fitted(reg)+ep.star
# Do bootstrap regression
x <- model.frame(reg)[,2]
bs.reg <- lm(y.star~x)
# Create bootstrapped adjusted residuals
bs.lev <- influence(bs.reg)$hat
bs.s   <- residuals(bs.reg)/sqrt(1-bs.lev)
bs.s   <- bs.s - mean(bs.s)
# Calculate draw on prediction error
xb.xb <- coef(my.reg)["(Intercept)"] - coef(bs.reg)["(Intercept)"]
xb.xb <- xb.xb + (coef(my.reg)["x"] - coef(bs.reg)["x"])*x_Np1
return(unname(xb.xb + sample(bs.s,size=1)))
}
# Do bootstrap with 10,000 replications
ep.draws <- replicate(n=10000,the.replication(reg=my.reg,s=my.s.resid,x_Np1=78))
# Create prediction interval
y.p+quantile(ep.draws,probs=c(0.05,0.95))
# prediction interval using normal assumption
predict(my.reg,newdata=data.frame(x=54.2857143),interval="prediction",level=0.90)
library(MASS)
library(Hmisc)
my.reg <- lm(y ~ x + poly(x,degree=2), data = df)
# Predict y for x=78:
y.p <- coef(my.reg)["(Intercept)"] + coef(my.reg)["x"]*54.2857143
y.p
# Create adjusted residuals
leverage <- influence(my.reg)$hat
my.s.resid <- residuals(my.reg)/sqrt(1-leverage)
my.s.resid <- my.s.resid - mean(my.s.resid)
reg <- my.reg
s <- my.s.resid
the.replication <- function(reg,s,x_Np1=0){
# Make bootstrap residuals
ep.star <- sample(s,size=length(reg$residuals),replace=TRUE)
# Make bootstrap Y
y.star <- fitted(reg)+ep.star
# Do bootstrap regression
x <- model.frame(reg)[,2]
bs.reg <- lm(y.star~x)
# Create bootstrapped adjusted residuals
bs.lev <- influence(bs.reg)$hat
bs.s   <- residuals(bs.reg)/sqrt(1-bs.lev)
bs.s   <- bs.s - mean(bs.s)
# Calculate draw on prediction error
xb.xb <- coef(my.reg)["(Intercept)"] - coef(bs.reg)["(Intercept)"]
xb.xb <- xb.xb + (coef(my.reg)["x"] - coef(bs.reg)["x"])*x_Np1
return(unname(xb.xb + sample(bs.s,size=1)))
}
# Do bootstrap with 10,000 replications
ep.draws <- replicate(n=10000,the.replication(reg=my.reg,s=my.s.resid,x_Np1=54.2857143))
# Create prediction interval
y.p+quantile(ep.draws,probs=c(0.05,0.95))
# prediction interval using normal assumption
predict(my.reg,newdata=data.frame(x=54.2857143),interval="prediction",level=0.90)
library(MASS)
library(Hmisc)
my.reg <- lm(y ~ x + poly(x,degree=2), data = df)
# Predict y for x=78:
y.p <- coef(my.reg)["(Intercept)"] + coef(my.reg)["x"]*54.2857143
y.p
# Create adjusted residuals
leverage <- influence(my.reg)$hat
my.s.resid <- residuals(my.reg)/sqrt(1-leverage)
my.s.resid <- my.s.resid - mean(my.s.resid)
reg <- my.reg
s <- my.s.resid
the.replication <- function(reg,s,x_Np1=0){
# Make bootstrap residuals
ep.star <- sample(s,size=length(reg$residuals),replace=TRUE)
# Make bootstrap Y
y.star <- fitted(reg)+ep.star
# Do bootstrap regression
x <- model.frame(reg)[,2]
bs.reg <- lm(y.star~x)
# Create bootstrapped adjusted residuals
bs.lev <- influence(bs.reg)$hat
bs.s   <- residuals(bs.reg)/sqrt(1-bs.lev)
bs.s   <- bs.s - mean(bs.s)
# Calculate draw on prediction error
xb.xb <- coef(my.reg)["(Intercept)"] - coef(bs.reg)["(Intercept)"]
xb.xb <- xb.xb + (coef(my.reg)["x"] - coef(bs.reg)["x"])*x_Np1
return(unname(xb.xb + sample(bs.s,size=1)))
}
# Do bootstrap with 1000 replications
ep.draws <- replicate(n=1000,the.replication(reg=my.reg,s=my.s.resid,x_Np1=54.2857143))
# Create prediction interval
y.p+quantile(ep.draws,probs=c(0.05,0.95))
# prediction interval using normal assumption
predict(my.reg,newdata=data.frame(x=54.2857143),interval="prediction",level=0.90)
library(MASS)
library(Hmisc)
my.reg <- lm(y ~ x + poly(x,degree=2), data = df)
# Predict y for x=78:
y.p <- coef(my.reg)["(Intercept)"] + coef(my.reg)["x"]*54.2857143
y.p
# Create adjusted residuals
leverage <- influence(my.reg)$hat
my.s.resid <- residuals(my.reg)/sqrt(1-leverage)
my.s.resid <- my.s.resid - mean(my.s.resid)
reg <- my.reg
s <- my.s.resid
the.replication <- function(reg,s,x_Np1=0){
# Make bootstrap residuals
ep.star <- sample(s,size=length(reg$residuals),replace=TRUE)
# Make bootstrap Y
y.star <- fitted(reg)+ep.star
# Do bootstrap regression
x <- model.frame(reg)[,2]
bs.reg <- lm(y.star~x)
# Create bootstrapped adjusted residuals
bs.lev <- influence(bs.reg)$hat
bs.s   <- residuals(bs.reg)/sqrt(1-bs.lev)
bs.s   <- bs.s - mean(bs.s)
# Calculate draw on prediction error
xb.xb <- coef(my.reg)["(Intercept)"] - coef(bs.reg)["(Intercept)"]
xb.xb <- xb.xb + (coef(my.reg)["x"] - coef(bs.reg)["x"])*x_Np1
return(unname(xb.xb + sample(bs.s,size=1)))
}
# Do bootstrap with 1000 replications
ep.draws <- replicate(n=10000,the.replication(reg=my.reg,s=my.s.resid,x_Np1=54.2857143))
# Create prediction interval
y.p+quantile(ep.draws,probs=c(0.05,0.95))
# prediction interval using normal assumption
predict(my.reg,newdata=data.frame(x=54.2857143),interval="prediction",level=0.90)
library(MASS)
library(Hmisc)
my.reg <- lm(y ~ x + poly(x,degree=2), data = df)
# Predict y for x=78:
y.p <- coef(my.reg)["(Intercept)"] + coef(my.reg)["x"]*54.2857143
y.p
# Create adjusted residuals
leverage <- influence(my.reg)$hat
my.s.resid <- residuals(my.reg)/sqrt(1-leverage)
my.s.resid <- my.s.resid - mean(my.s.resid)
reg <- my.reg
s <- my.s.resid
the.replication <- function(reg,s,x_Np1=0){
# Make bootstrap residuals
ep.star <- sample(s,size=length(reg$residuals),replace=TRUE)
# Make bootstrap Y
y.star <- fitted(reg)+ep.star
# Do bootstrap regression
x <- model.frame(reg)[,2]
bs.reg <- lm(y.star~x)
# Create bootstrapped adjusted residuals
bs.lev <- influence(bs.reg)$hat
bs.s   <- residuals(bs.reg)/sqrt(1-bs.lev)
bs.s   <- bs.s - mean(bs.s)
# Calculate draw on prediction error
xb.xb <- coef(my.reg)["(Intercept)"] - coef(bs.reg)["(Intercept)"]
xb.xb <- xb.xb + (coef(my.reg)["x"] - coef(bs.reg)["x"])*x_Np1
return(unname(xb.xb + sample(bs.s,size=1)))
}
# Do bootstrap with 1000 replications
ep.draws <- replicate(n=100,the.replication(reg=my.reg,s=my.s.resid,x_Np1=54.2857143))
# Create prediction interval
y.p+quantile(ep.draws,probs=c(0.05,0.95))
# prediction interval using normal assumption
predict(my.reg,newdata=data.frame(x=54.2857143),interval="prediction",level=0.90)
library(MASS)
library(Hmisc)
my.reg <- lm(y ~ x + poly(x,degree=2), data = df)
# Predict y for x=78:
y.p <- coef(my.reg)["(Intercept)"] + coef(my.reg)["x"]*54.2857143
y.p
# Create adjusted residuals
leverage <- influence(my.reg)$hat
my.s.resid <- residuals(my.reg)/sqrt(1-leverage)
my.s.resid <- my.s.resid - mean(my.s.resid)
reg <- my.reg
s <- my.s.resid
the.replication <- function(reg,s,x_Np1=0){
# Make bootstrap residuals
ep.star <- sample(s,size=length(reg$residuals),replace=TRUE)
# Make bootstrap Y
y.star <- fitted(reg)+ep.star
# Do bootstrap regression
x <- model.frame(reg)[,2]
bs.reg <- lm(y.star~x)
# Create bootstrapped adjusted residuals
bs.lev <- influence(bs.reg)$hat
bs.s   <- residuals(bs.reg)/sqrt(1-bs.lev)
bs.s   <- bs.s - mean(bs.s)
# Calculate draw on prediction error
xb.xb <- coef(my.reg)["(Intercept)"] - coef(bs.reg)["(Intercept)"]
xb.xb <- xb.xb + (coef(my.reg)["x"] - coef(bs.reg)["x"])*x_Np1
return(unname(xb.xb + sample(bs.s,size=1)))
}
# Do bootstrap with 1000 replications
ep.draws <- replicate(n=1000,the.replication(reg=my.reg,s=my.s.resid,x_Np1=54.2857143))
# Create prediction interval
y.p+quantile(ep.draws,probs=c(0.05,0.95))
# prediction interval using normal assumption
predict(my.reg,newdata=data.frame(x=54.2857143),interval="prediction",level=0.90)
library(MASS)
library(Hmisc)
my.reg <- lm(y ~ x + poly(x,degree=2), data = df)
# Predict y for x=78:
y.p <- coef(my.reg)["(Intercept)"] + coef(my.reg)["x"]*54.2857143
y.p
# Create adjusted residuals
leverage <- influence(my.reg)$hat
my.s.resid <- residuals(my.reg)/sqrt(1-leverage)
my.s.resid <- my.s.resid - mean(my.s.resid)
reg <- my.reg
s <- my.s.resid
the.replication <- function(reg,s,x_Np1=0){
# Make bootstrap residuals
ep.star <- sample(s,size=length(reg$residuals),replace=TRUE)
# Make bootstrap Y
y.star <- fitted(reg)+ep.star
# Do bootstrap regression
x <- model.frame(reg)[,2]
bs.reg <- lm(y.star~x)
# Create bootstrapped adjusted residuals
bs.lev <- influence(bs.reg)$hat
bs.s   <- residuals(bs.reg)/sqrt(1-bs.lev)
bs.s   <- bs.s - mean(bs.s)
# Calculate draw on prediction error
xb.xb <- coef(my.reg)["(Intercept)"] - coef(bs.reg)["(Intercept)"]
xb.xb <- xb.xb + (coef(my.reg)["x"] - coef(bs.reg)["x"])*x_Np1
return(unname(xb.xb + sample(bs.s,size=1)))
}
# Do bootstrap with 1000 replications
ep.draws <- replicate(n=1000,the.replication(reg=my.reg,s=my.s.resid,x_Np1=54.2857143))
# Create prediction interval
y.p+quantile(ep.draws,probs=c(0.05,0.95))
library(MASS)
library(Hmisc)
my.reg <- lm(y ~ x + poly(x,degree=2), data = df)
# Predict y for x=78:
y.p <- coef(my.reg)["(Intercept)"] + coef(my.reg)["x"]*54.2857143
y.p
# Create adjusted residuals
leverage <- influence(my.reg)$hat
my.s.resid <- residuals(my.reg)/sqrt(1-leverage)
my.s.resid <- my.s.resid - mean(my.s.resid)
reg <- my.reg
s <- my.s.resid
the.replication <- function(reg,s,x_Np1=0){
# Make bootstrap residuals
ep.star <- sample(s,size=length(reg$residuals),replace=TRUE)
# Make bootstrap Y
y.star <- fitted(reg)+ep.star
# Do bootstrap regression
x <- model.frame(reg)[,2]
bs.reg <- lm(y.star~x)
# Create bootstrapped adjusted residuals
bs.lev <- influence(bs.reg)$hat
bs.s   <- residuals(bs.reg)/sqrt(1-bs.lev)
bs.s   <- bs.s - mean(bs.s)
# Calculate draw on prediction error
xb.xb <- coef(my.reg)["(Intercept)"] - coef(bs.reg)["(Intercept)"]
xb.xb <- xb.xb + (coef(my.reg)["x"] - coef(bs.reg)["x"])*x_Np1
return(unname(xb.xb + sample(bs.s,size=1)))
}
# Do bootstrap with 1000 replications
ep.draws <- replicate(n=1000,the.replication(reg=my.reg,s=my.s.resid,x_Np1=54.2857143))
# Create prediction interval
y.p+quantile(ep.draws,probs=c(0.05,0.95))
# prediction interval using normal assumption
predict(my.reg,newdata=data.frame(x=54.2857143),interval="prediction",level=0.90)
library(dplyr)
library(janitor)
library(reshape2)
df <- read.csv("Austin_Water_-_Residential_Water_Consumption.csv")
df <- clean_names(df)
df$year <- substr(df$year_month, 1, 4)
df$year <- as.numeric(df$year)
df$month <- substr(df$year_month, 5, 6)
df$month <- as.factor(df$month)
df <- dplyr::select(df, -c(year_month))
df$customer_class <- gsub(" - ", "_", df$customer_class)
df <- filter(df, postal_code != "")
df$total_gallons[is.na(df$total_gallons)] <- 0
dfwide <- dcast(data=df, postal_code+month+year~customer_class, value.var= 'total_gallons')
dfwide <- clean_names(dfwide)
df$total_gallons[is.na(df$total_gallons)] <- 0
dfwide[is.na(dfwide)] <- 0
library(ggplot2)
options(scipen=999)
print(summary(dfwide))
total_water <- df %>% group_by(year, customer_class) %>%
summarize(total_gallons = sum(total_gallons))
tw <- ggplot(total_water, aes(x=year, y=total_gallons, group=customer_class, color=customer_class)) +
geom_line()
print(tw)
library(caTools)
logdf <- dfwide
for (i in 1:length(logdf$irrigation_residential)){
if(logdf$irrigation_residential[i] > mean(logdf$irrigation_residential)){
logdf$HiResIrr[i] <- 1
}
else if(logdf$irrigation_residential[i] <= mean(logdf$irrigation_residential)){
logdf$HiResIrr[i] <- 0
}
}
set.seed(88)
split = sample.split(logdf$HiResIrr, SplitRatio = 0.75)
logdf <- dplyr::select(logdf, -c(irrigation_residential, postal_code))
train = subset(logdf, split == TRUE)
test = subset(logdf, split == FALSE)
model <- glm(HiResIrr ~.,family=binomial(link='logit'),data=train)
summary(model)
library(ggplot2)
set.seed(666)
x <- rnorm(100, 75, 20)
x2 <- x*x
e <- rnorm(100, 0, 7)
y <- 125 + -3.8*x + 0.035*x**2
df <- data.frame(y, x, x2)
ggplot(data = df, aes(x=x, y=y)) + geom_point() + geom_vline(xintercept = 54.2857143, linetype = "dashed", color = "red")
library(boot)
model <- lm(y ~ x + x2, data = df)
summary(model)
library(boot)
bs <- function(data, indices) {
d <- data[indices,] # allows boot to select sample
fit <- lm(y ~ x + x2, data=d)
return(coef(fit))
return(-coef(fit)["x"]/(2*coef(fit)["x2"]))
}
results <- boot(data=mtcars, statistic=bs,
R=1000)
library(boot)
bs <- function(data, indices) {
d <- data[indices,] # allows boot to select sample
fit <- lm(y ~ x + x2, data=d)
return(coef(fit))
return(-coef(fit)["x"]/(2*coef(fit)["x2"]))
}
results <- boot(data=mtcars, statistic=bs,
R=1000)
results
library(boot)
bs <- function(data, indices) {
d <- data[indices,] # allows boot to select sample
fit <- lm(y ~ x + x2, data=d)
return(-coef(fit)["x"]/(2*coef(fit)["x2"]))
}
results <- boot(data=mtcars, statistic=bs,
R=1000)
results
library(boot)
bs <- function(data, indices) {
d <- data[indices,] # allows boot to select sample
fit <- lm(y ~ x + x2, data=d)
return(-coef(fit)["x"]/(2*coef(fit)["x2"]))
}
results <- boot(data=mtcars, statistic=bs,
R=10)
results
library(boot)
bs <- function(data, indices) {
d <- data[indices,] # allows boot to select sample
fit <- lm(y ~ x + x2, data=d)
return(-coef(fit)["x"]/(2*coef(fit)["x2"]))
}
results <- boot(data=mtcars, statistic=bs,
R=1000)
results
library(boot)
bs <- function(data, indices) {
d <- data[indices,] # allows boot to select sample
fit <- lm(y ~ x + x2, data=d)
return(-coef(fit)["x"]/(2*coef(fit)["x2"]))
}
results <- boot(data=df, statistic=bs,
R=1000)
results
?boot
library(ggplot2)
set.seed(666)
x <- rnorm(100, 75, 20)
x2 <- x*x
e <- rnorm(100, 0, 7)
y <- 125 + -3.8*x + 0.035*x**2
df <- data.frame(y, x, x2)
ggplot(data = df, aes(x=x, y=y)) + geom_point() + geom_vline(xintercept = 54.2857143, linetype = "dashed", color = "red")
library(boot)
model <- lm(y ~ x + x2, data = df)
summary(model)
library(boot)
bs <- function(data, indices) {
d <- data[indices,] # allows boot to select sample
fit <- lm(y ~ x + x2, data=d)
return(-coef(fit)["x"]/(2*coef(fit)["x2"]))
}
results <- boot(data=df, statistic=bs,
R=1000)
results
plot(results, index=2)
plot(results, index=x)
library(boot)
bs <- function(data, indices) {
d <- data[indices,] # allows boot to select sample
fit <- lm(y ~ x + x2, data=d)
return(-coef(fit)["x"]/(2*coef(fit)["x2"]))
}
results <- boot(data=df, statistic=bs(df, 1:100),
R=1000)
library(boot)
bs <- function(data, indices) {
d <- data[indices,] # allows boot to select sample
fit <- lm(y ~ x + x2, data=d)
return(-coef(fit)["x"]/(2*coef(fit)["x2"]))
}
results <- boot(data=df, statistic=bs,
R=1000)
results
bs(data=df, 1:100)
View(results)
View(df)
results <- boot(data=df, statistic=bs,
R=1000)
results
library(boot)
bs <- function(data, indices) {
d <- data[indices,] # allows boot to select sample
fit <- lm(y ~ x + x2, data=d)
return(-coef(fit)["x"]/(2*coef(fit)["x2"]))
}
results <- boot(data=df, statistic=bs,
R=1000)
results
library(boot)
model <- lm(y ~ x + x2, data = df)
summary(model)
library(boot)
model <- lm(y ~ x + x2, data = df)
summary(model)
newdata <- data.frame(x=54.2857143)
predict(model, newdata,interval="prediction")
library(boot)
model <- lm(y ~ x + x2, data = df)
summary(model)
library(boot)
bs <- function(data, indices) {
d <- data[indices,] # allows boot to select sample
fit <- lm(y ~ x + x2, data=d, subset= indices)
return(-coef(fit)["x"]/(2*coef(fit)["x2"]))
}
results <- boot(data=df, statistic=bs,
R=1000)
library(ggplot2)
set.seed(666)
x <- rnorm(100, 75, 20)
x2 <- x*x
e <- rnorm(100, 0, 7)
y <- 125 + -3.8*x + 0.035*x**2
df <- data.frame(y, x, x2)
ggplot(data = df, aes(x=x, y=y)) + geom_point() + geom_vline(xintercept = 54.2857143, linetype = "dashed", color = "red")
library(boot)
model <- lm(y ~ x + x2, data = df)
summary(model)
library(boot)
bs <- function(data, indices) {
d <- data[indices,] # allows boot to select sample
fit <- lm(y ~ x + x2, data=d, subset= indices)
return(-coef(fit)["x"]/(2*coef(fit)["x2"]))
}
results <- boot(data=df, statistic=bs,
R=1000)
results
set.seed(2)
rand <- sample(nrow(atx))
k1row <- rand[rand %% 5 + 1 == 1]
k2row <- rand[rand %% 5 + 1 == 2]
k3row <- rand[rand %% 5 + 1 == 3]
k4row <- rand[rand %% 5 + 1 == 4]
k5row <- rand[rand %% 5 + 1 == 5]
k1fold <- atx[k1row,]
k2fold <- atx[k2row,]
k3fold <- atx[k3row,]
k4fold <- atx[k4row,]
k5fold <- atx[k5row,]
print("Summary Statistics for 5 folds")
summary(k1fold$hi_use)
summary(k2fold$hi_use)
summary(k3fold$hi_use)
summary(k4fold$hi_use)
summary(k5fold$hi_use)
#model with k1fold as test
model <- glm(hi_use~ customer_class +median_home_value + median_household_income + median_rent ,family=binomial(link='logit'),data=rbind(k2fold, k3fold,k4fold,k5fold))
p <- predict(model, k1fold, type = "response")
p_class <- ifelse(p > 0.5, "Predict 1", "Predict 0")
print(table(p_class, k1fold[["hi_use"]]))
#model with k2fold as test
model <- glm(hi_use~ customer_class +median_home_value + median_household_income + median_rent ,family=binomial(link='logit'),data=rbind(k1fold, k3fold,k4fold,k5fold))
p <- predict(model, k2fold, type = "response")
p_class <- ifelse(p > 0.5, "Predict 1", "Predict 0")
print(table(p_class, k2fold[["hi_use"]]))
#Model with k3fold as test
model <- glm(hi_use~ customer_class +median_home_value + median_household_income + median_rent ,family=binomial(link='logit'),data=rbind(k2fold, k1fold,k4fold,k5fold))
p <- predict(model, k3fold, type = "response")
p_class <- ifelse(p > 0.5, "Predict 1", "Predict 0")
print(table(p_class, k3fold[["hi_use"]]))
#Model with k4fold as test
model <- glm(hi_use~ customer_class +median_home_value + median_household_income + median_rent ,family=binomial(link='logit'),data=rbind(k2fold, k1fold,k3fold,k5fold))
p <- predict(model, k4fold, type = "response")
p_class <- ifelse(p > 0.5, "Predict 1", "Predict 0")
print(table(p_class, k4fold[["hi_use"]]))
#Model with k5fold as test
model <- glm(hi_use~ customer_class +median_home_value + median_household_income + median_rent ,family=binomial(link='logit'),data=rbind(k2fold, k1fold,k3fold,k4fold))
p <- predict(model, k5fold, type = "response")
p_class <- ifelse(p > 0.5, "Predict 1", "Predict 0")
print(table(p_class, k5fold[["hi_use"]]))
