#test error
y_test <- test[, 11]
pcr_pred <- predict(pcr_model, test, ncomp = 7)
mean((pcr_pred - y_test)^2)
require(pls)
library(caTools)
set.seed (27)
split = sample.split(simdata$simY, SplitRatio = 0.80)
train = subset(simdata, split == TRUE)
test = subset(simdata, split == FALSE)
pcr_model <- pls::pcr(simY~., data = train, scale = TRUE, validation = "CV")
summary(pcr_model)
# Plot the cross validation MSE
validationplot(pcr_model, val.type="MSEP")
#test error
y_test <- test[, 11]
pcr_pred <- predict(pcr_model, test, ncomp = 9)
mean((pcr_pred - y_test)^2)
pls_model <- pls::plsr(simY~., data = train, scale = TRUE, validation = "CV")
summary(pls_model)
# Plot the cross validation MSE
validationplot(pls_model, val.type="MSEP")
#test error
y_test <- test[, 11]
pcr_pred <- predict(pls_model, test, ncomp = 9)
mean((pcr_pred - y_test)^2)
pls_model <- pls::plsr(simY~., data = train, scale = TRUE, validation = "CV")
summary(pls_model)
# Plot the cross validation MSE
validationplot(pls_model, val.type="MSEP")
#test error
y_test <- test[, 11]
pcr_pred <- predict(pls_model, test, ncomp = 6)
mean((pcr_pred - y_test)^2)
pls_model <- pls::plsr(simY~., data = train, scale = TRUE, validation = "CV")
summary(pls_model)
# Plot the cross validation MSE
validationplot(pls_model, val.type="MSEP")
#test error
y_test <- test[, 11]
pcr_pred <- predict(pls_model, test, ncomp = 3)
mean((pcr_pred - y_test)^2)
pls_model <- pls::plsr(simY~., data = train, scale = TRUE, validation = "CV")
summary(pls_model)
# Plot the cross validation MSE
validationplot(pls_model, val.type="MSEP")
#test error
y_test <- test[, 11]
pcr_pred <- predict(pls_model, test, ncomp = 4)
mean((pcr_pred - y_test)^2)
pls_model <- pls::plsr(simY~., data = train, scale = TRUE, validation = "CV")
summary(pls_model)
# Plot the cross validation MSE
validationplot(pls_model, val.type="MSEP")
#test error
y_test <- test[, 11]
pcr_pred <- predict(pls_model, test, ncomp = 5)
mean((pcr_pred - y_test)^2)
pls_model <- pls::plsr(simY~., data = train, scale = TRUE, validation = "CV")
summary(pls_model)
# Plot the cross validation MSE
validationplot(pls_model, val.type="MSEP")
#test error
y_test <- test[, 11]
pcr_pred <- predict(pls_model, test, ncomp = 4)
mean((pcr_pred - y_test)^2)
pls_model <- pls::plsr(simY~., data = train, scale = TRUE, validation = "CV")
summary(pls_model)
# Plot the cross validation MSE
validationplot(pls_model, val.type="MSEP")
#test error
y_test <- test[, 11]
pcr_pred <- predict(pls_model, test, ncomp = 2)
mean((pcr_pred - y_test)^2)
pls_model <- pls::plsr(simY~., data = train, scale = TRUE, validation = "CV")
summary(pls_model)
# Plot the cross validation MSE
validationplot(pls_model, val.type="MSEP")
#test error
y_test <- test[, 11]
pcr_pred <- predict(pls_model, test, ncomp = 3)
mean((pcr_pred - y_test)^2)
pls_model <- pls::plsr(simY~., data = train, scale = TRUE, validation = "CV")
summary(pls_model)
# Plot the cross validation MSE
validationplot(pls_model, val.type="MSEP")
#test error
y_test <- test[, 11]
pcr_pred <- predict(pls_model, test, ncomp = 4)
mean((pcr_pred - y_test)^2)
simdata_812 <- data_gen(n=100, g=8, p_g = 12)
model812<-lm(simY~., data = train)
summary(model812)
simdata_812 <- data_gen(n=100, g=8, p_g = 12)
model812<-lm(simY~., data = train)
summary(model812)
simdata_812 <- data_gen(n=100, g=8, p_g = 12)
model812<-lm(simY~., data = train)
summary(model812)
#test error
lm812_pred <- predict(model812, test)
mean((pcr_pred - y_test)^2)
simdata_1212 <- data_gen(n=100, g=12, p_g = 12)
model1212<-lm(simY~., data = train)
summary(model1212)
#test error
lm812_pred <- predict(model1212, test)
mean((pcr_pred - y_test)^2)
simdata_812 <- data_gen(n=100, g=8, p_g = 12)
model812<-lm(simY~., data = train)
summary(model812)
#test error
lm812_pred <- predict(model812, test)
mean((pcr_pred - y_test)^2)
simdata_812 <- data_gen(n=100, g=8, p_g = 12)
model812<-lm(simY~., data = train)
summary(model812)
#test error
lm812_pred <- predict(model812, test)
mean((lm812_pred - y_test)^2)
simdata_1212 <- data_gen(n=100, g=12, p_g = 12)
model1212<-lm(simY~., data = train)
summary(model1212)
#test error
lm1212_pred <- predict(model1212, test)
mean((lm1212_pred - y_test)^2)
library(MASS)
data_gen <- function (n, g, p_g) {
simX <<- data.frame(ID=seq(1:n))
sum_g <- data.frame(ID=seq(1:n))
sapply(seq(1:g), function (i) {
temp_p <- qr.Q(qr(matrix(rnorm(p_g^2), p_g)))
Sigma_g <- abs(crossprod(temp_p, temp_p*(p_g:1)))
simX_tmp <- as.data.frame(mvrnorm(n=n, mu=runif(p_g, min = 0, max = 10), Sigma=Sigma_g))
colnames(simX_tmp) <- paste0("g", i, "q", seq(1:p_g))
simX <<- cbind(simX, simX_tmp)
sum_g <<- cbind(sum_g, rowSums(simX_tmp))
})
colnames(sum_g) <- c("ID",seq(1:g))
betas <<- rnorm(g+1, 0, 1)*10
error <<- rnorm(n, 0, 5)
simY <<- rowSums(t(t(sum_g[2:ncol(sum_g)]) * betas[2:length(betas)])) + betas[1] + error
simData <<- cbind(simX, simY)
}
simdata <- data_gen(n=1000, g=3, p_g = 3)
res <- cor(simdata)
library(corrplot)
corrplot(res, type = "upper", order = "hclust",
tl.col = "black", tl.srt = 45)
require(pls)
library(caTools)
set.seed (27)
split = sample.split(simdata$simY, SplitRatio = 0.80)
train = subset(simdata, split == TRUE)
test = subset(simdata, split == FALSE)
pcr_model <- pls::pcr(simY~., data = train, scale = TRUE, validation = "CV")
summary(pcr_model)
# Plot the cross validation MSE
validationplot(pcr_model, val.type="MSEP")
#test error
y_test <- test[, 11]
pcr_pred <- predict(pcr_model, test, ncomp = 9)
mean((pcr_pred - y_test)^2)
pls_model <- pls::plsr(simY~., data = train, scale = TRUE, validation = "CV")
summary(pls_model)
# Plot the cross validation MSE
validationplot(pls_model, val.type="MSEP")
#test error
y_test <- test[, 11]
pcr_pred <- predict(pls_model, test, ncomp = 4)
mean((pcr_pred - y_test)^2)
simdata_812 <- data_gen(n=100, g=8, p_g = 12)
model812<-lm(simY~., data = train)
summary(model812)
#test error
lm812_pred <- predict(model812, test)
mean((lm812_pred - y_test)^2)
simdata_1212 <- data_gen(n=100, g=12, p_g = 12)
model1212<-lm(simY~., data = train)
summary(model1212)
#test error
lm1212_pred <- predict(model1212, test)
mean((lm1212_pred - y_test)^2)
simdata_812 <- data_gen(n=100, g=8, p_g = 12)
split = sample.split(simdata_812$simY, SplitRatio = 0.80)
train = subset(simdata, split == TRUE)
test = subset(simdata, split == FALSE)
model812<-lm(simY~., data = train)
summary(model812)
#test error
lm812_pred <- predict(model812, test)
mean((lm812_pred - y_test)^2)
simdata_1212 <- data_gen(n=100, g=12, p_g = 12)
split = sample.split(simdata_1212$simY, SplitRatio = 0.80)
model1212<-lm(simY~., data = train)
summary(model1212)
#test error
lm1212_pred <- predict(model1212, test)
mean((lm1212_pred - y_test)^2)
simdata_1212 <- data_gen(n=100, g=12, p_g = 12)
split = sample.split(simdata_1212$simY, SplitRatio = 0.80)
train = subset(simdata, split == TRUE)
test = subset(simdata, split == FALSE)
model1212<-lm(simY~., data = train)
summary(model1212)
#test error
lm1212_pred <- predict(model1212, test)
mean((lm1212_pred - y_test)^2)
simdata_812 <- data_gen(n=100, g=8, p_g = 12)
split = sample.split(simdata_812$simY, SplitRatio = 0.80)
train = subset(simdata, split == TRUE)
test = subset(simdata, split == FALSE)
model812<-lm(simY~., data = train)
summary(model812)
#test error
lm812_pred <- predict(model812, test)
mean((lm812_pred - y_test)^2)
set.seed (27)
simdata_812 <- data_gen(n=100, g=8, p_g = 12)
split = sample.split(simdata_812$simY, SplitRatio = 0.80)
train = subset(simdata, split == TRUE)
test = subset(simdata, split == FALSE)
model812<-lm(simY~., data = train)
summary(model812)
#test error
lm812_pred <- predict(model812, test)
mean((lm812_pred - y_test)^2)
set.seed (27)
simdata_1212 <- data_gen(n=100, g=12, p_g = 12)
split = sample.split(simdata_1212$simY, SplitRatio = 0.80)
train = subset(simdata, split == TRUE)
test = subset(simdata, split == FALSE)
model1212<-lm(simY~., data = train)
summary(model1212)
#test error
lm1212_pred <- predict(model1212, test)
mean((lm1212_pred - y_test)^2)
set.seed(27)
simdata <- data_gen(n=1000, g=3, p_g = 3)
res <- cor(simdata)
library(corrplot)
corrplot(res, type = "upper", order = "hclust",
tl.col = "black", tl.srt = 45)
simdata <- data_gen(n=1000, g=3, p_g = 3)
res <- cor(simdata)
library(corrplot)
corrplot(res, type = "upper", order = "hclust",
tl.col = "black", tl.srt = 45)
library(MASS)
data_gen <- function (n, g, p_g) {
simX <<- data.frame(ID=seq(1:n))
sum_g <- data.frame(ID=seq(1:n))
sapply(seq(1:g), function (i) {
temp_p <- qr.Q(qr(matrix(rnorm(p_g^2), p_g)))
Sigma_g <- abs(crossprod(temp_p, temp_p*(p_g:1)))
simX_tmp <- as.data.frame(mvrnorm(n=n, mu=runif(p_g, min = 0, max = 10), Sigma=Sigma_g))
colnames(simX_tmp) <- paste0("g", i, "q", seq(1:p_g))
simX <<- cbind(simX, simX_tmp)
sum_g <<- cbind(sum_g, rowSums(simX_tmp))
})
colnames(sum_g) <- c("ID",seq(1:g))
betas <<- rnorm(g+1, 0, 1)*10
error <<- rnorm(n, 0, 5)
simY <<- rowSums(t(t(sum_g[2:ncol(sum_g)]) * betas[2:length(betas)])) + betas[1] + error
simData <<- cbind(simX, simY)
}
set.seed(27)
simdata <- data_gen(n=1000, g=3, p_g = 3)
res <- cor(simdata)
library(corrplot)
corrplot(res, type = "upper", order = "hclust",
tl.col = "black", tl.srt = 45)
?corrplot
set.seed(27)
simdata <- data_gen(n=1000, g=3, p_g = 3)
res <- cor(simdata)
library(corrplot)
corrplot(res, type = "upper", order = "hclust",
tl.col = "black")
res
set.seed(27)
simdata <- data_gen(n=1000, g=3, p_g = 3)
res <- cor(simdata)
library(corrplot)
corrplot::corrplot(res, type = "upper", order = "hclust",
tl.col = "black")
set.seed(27)
simdata <- data_gen(n=1000, g=3, p_g = 3)
res <- cor(simdata)
library(corrplot)
corrplot::corrplot(res, type = "upper", order = "hclust",
tl.col = "black")
set.seed(55)
simdata <- data_gen(n=1000, g=3, p_g = 3)
res <- cor(simdata)
library(corrplot)
corrplot::corrplot(res, type = "upper", order = "hclust",
tl.col = "black")
set.seed(88)
simdata <- data_gen(n=1000, g=3, p_g = 3)
res <- cor(simdata)
library(corrplot)
corrplot::corrplot(res, type = "upper", order = "hclust",
tl.col = "black")
set.seed(666)
simdata <- data_gen(n=1000, g=3, p_g = 3)
res <- cor(simdata)
library(corrplot)
corrplot::corrplot(res, type = "upper", order = "hclust",
tl.col = "black")
set.seed(1000)
simdata <- data_gen(n=1000, g=3, p_g = 3)
res <- cor(simdata)
library(corrplot)
corrplot::corrplot(res, type = "upper", order = "hclust",
tl.col = "black")
library(caret)
library(leaps)
library(MASS)
# Fit the full model
#full.model <- lm(Mail_Return_Rate_CEN_2010 ~., data = train)
# Stepwise regression model
#step.model <- stepAIC(full.model, direction = "forward", steps = 10,
#                      trace = FALSE)
#summary(step.model)
#models <- regsubsets(Mail_Return_Rate_CEN_2010~., data = train, nvmax = 10, method = "forward")
#summary(models)
library(olsrr)
model<-lm(Mail_Return_Rate_CEN_2010~., data = train)
library(readr)
library(dplyr)
library(caTools)
set.seed(27)
data <- read_csv("pdb2016bgv8_us_clean (1).csv")
data <- filter(data, data$State_name == "Texas" & data$County_name == "Travis County")
drop_columns <- c("TEA_Update_Leave_CEN_2010", "BILQ_Mailout_count_CEN_2010", "BILQ_Frms_CEN_2010",
"pct_TEA_Update_Leave_CEN_2010", "pct_BILQ_Mailout_count_CEN_2010", "Low_Response_Score",
"Census_Mail_Returns_CEN_2010")
data <- data[ , !(names(data) %in% drop_columns)]
data_noID <- data[9:ncol(data)]
drop_LD_columns <- c("AIAN_LAND", "Tot_Population_CEN_2010", "Females_CEN_2010", "Females_ACS_10_14",
"Pop_65plus_CEN_2010", "Pop_65plus_ACS_10_14", "Non_Inst_GQ_CEN_2010", "Pop_5yrs_Over_ACS_10_14",
"Pop_25yrs_Over_ACS_10_14", "ENG_VW_ACS_10_14", "NonFamily_HHD_CEN_2010", "NonFamily_HHD_ACS_10_14",
"Tot_Prns_in_HHD_CEN_2010", "Tot_Occp_Units_CEN_2010", "Tot_Occp_Units_ACS_10_14", "Tot_Vacant_Units_CEN_2010",
"Tot_Vacant_Units_ACS_10_14", "Owner_Occp_HU_CEN_2010", "Owner_Occp_HU_ACS_10_14",
"Valid_Mailback_Count_CEN_2010", "pct_Females_CEN_2010", "pct_Pop_5yrs_Over_ACS_10_14",
"pct_Not_MrdCple_HHD_CEN_2010", "pct_Vacant_Units_ACS_10_14", "pct_Owner_Occp_HU_CEN_2010",
"pct_TEA_MailOutMailBack_CEN_2010", "pct_Deletes_CEN_2010")
data_noID_noMC <- data_noID [ , !(names(data_noID) %in% drop_LD_columns)]
data_noID_noMC <- data_noID_noMC [, -grep("pct_", colnames(data_noID_noMC))]
split = sample.split(data_noID_noMC$Mail_Return_Rate_CEN_2010, SplitRatio = 0.80)
train = subset(data_noID_noMC, split == TRUE)
test = subset(data_noID_noMC, split == FALSE)
library(caret)
library(leaps)
library(MASS)
# Fit the full model
#full.model <- lm(Mail_Return_Rate_CEN_2010 ~., data = train)
# Stepwise regression model
#step.model <- stepAIC(full.model, direction = "forward", steps = 10,
#                      trace = FALSE)
#summary(step.model)
#models <- regsubsets(Mail_Return_Rate_CEN_2010~., data = train, nvmax = 10, method = "forward")
#summary(models)
library(olsrr)
model<-lm(Mail_Return_Rate_CEN_2010~., data = train)
FWDfit.p<-ols_step_forward_p(model,penter=.0002)
#This gives you the short summary of the models at each step
print("Forward")
FWDfit.p
BWDfit.p <-ols_step_backward_p(model, prem = .000001)
#This gives you the short summary of the models at each step
print("Backward")
BWDfit.p
BWDfit.p
BWDfit.p$model
BWDfit.p$rsquare
BWDfit.p$indvar
View(BWDfit.p)
BWDfit.p$model$coefficients
knitr::opts_chunk$set(echo = TRUE)
# Clear environment
rm(list = ls())
library(tidyverse)
library(ISLR)
hit <- Hitters
hit <- na.omit(hit)
#Question: Lets continue working with the hitters dataset. Classify top 20% of the salaries as High and the rest as Low.  Select 5 variables from the  dataset that you believe will best help predict players' salary. Split the data into 60% training and 40% tet sets. Use these variables to grow a decision tree using the training set. Use cross-validation to prune the tree and find the best size. Finally, use the pruned tree to predict the salaries. Reapply the top20% salary classification for the predicted data, and print a confusion matrix for the training and test sets separately.
### Apply classification threshold ###
#Compare if salary is greater than the 80th quantile (ie, salary is in the top 20% of all salaries)
hit$SalaryClass <- ifelse(hit$Salary > quantile(hit$Salary, probs = 0.8), "High", "Low")
table(hit$SalaryClass) #Note that 20% of salaries have been classified as High, like we wanted
### Select 5 potential predictors ###
names(hit)
#Lets us say we use best subset selection to identify top 5 variables (mainly since I'm clueless about baseball)
##NOTE: FOR YOUR ASSIGNMENT,NO NEED TO DO SUBSET SELECTION. JUST SELECT APPROPRIATE VARIABLES BASED ON YOUR INTUITION.
library(leaps)
coef(regsubsets(Salary ~.-SalaryClass, hit, nvmax = 5),5)
#So we've identified AtBat, Hits, CRBI, Division, PutOuts - lets use these to grow our tree
### Split data into traiing/test index ###
set.seed(1234)
train <- sample(c(TRUE,FALSE), nrow(hit), rep=TRUE, prob = c(0.6,0.4))
test <- (!train)
### Growing a decision tree ###
library(tree)
install.packages("tree")
### Apply classification threshold ###
#Compare if salary is greater than the 80th quantile (ie, salary is in the top 20% of all salaries)
hit$SalaryClass <- ifelse(hit$Salary > quantile(hit$Salary, probs = 0.8), "High", "Low")
table(hit$SalaryClass) #Note that 20% of salaries have been classified as High, like we wanted
### Select 5 potential predictors ###
names(hit)
#Lets us say we use best subset selection to identify top 5 variables (mainly since I'm clueless about baseball)
##NOTE: FOR YOUR ASSIGNMENT,NO NEED TO DO SUBSET SELECTION. JUST SELECT APPROPRIATE VARIABLES BASED ON YOUR INTUITION.
library(leaps)
coef(regsubsets(Salary ~.-SalaryClass, hit, nvmax = 5),5)
#So we've identified AtBat, Hits, CRBI, Division, PutOuts - lets use these to grow our tree
### Split data into traiing/test index ###
set.seed(1234)
train <- sample(c(TRUE,FALSE), nrow(hit), rep=TRUE, prob = c(0.6,0.4))
test <- (!train)
### Growing a decision tree ###
library(tree)
mod.tree <- tree(Salary ~ AtBat + Hits + CRBI + Division + PutOuts, data = hit[train,]) #For your assignment, Here you may also need to specify mindev = 0.001and minsize = 5
summary(mod.tree)
plot(mod.tree)
text(mod.tree, pretty = 0)
### Prune tree with CV ###
#Find the best size of the tree - the point where deviance(dev) is lowest
mod.tree.cv <- cv.tree(mod.tree, FUN = prune.tree)
mod.tree.cv
plot(mod.tree.cv$size, mod.tree.cv$dev, type = "b")
#Seems like best size of the tree is 6 - lets use this to prune the tree
mod.tree.prune <- prune.tree(mod.tree, best = 6)
summary(mod.tree.prune)
plot(mod.tree.prune)
text(mod.tree.prune, pretty = 0)
### Predict salary, using the pruned tree ###
hit$Salary.Pred <- predict(mod.tree.prune, newdata = hit)
#Reapply 20% classification threshold for predicted salary
hit$Salary.Pred.Class <- ifelse(hit$Salary.Pred > quantile(hit$Salary.Pred, probs = 0.8), "High", "Low")
table(hit$Salary.Pred.Class)
### Confusion matrix ###
#For training set
table(TrueSalary = hit$SalaryClass[train], PredSalary = hit$Salary.Pred.Class[train]) #How many instances were misclassified? Error rates?
#For test set
table(TrueSalary = hit$SalaryClass[test], PredSalary = hit$Salary.Pred.Class[test]) #How many instances were misclassified? Error rates?
library(tree)
mod.tree <- tree(Salary ~ AtBat + Hits + CRBI + Division + PutOuts, data = hit[train,]) #For your assignment, Here you may also need to specify mindev = 0.001and minsize = 5
summary(mod.tree)
plot(mod.tree)
text(mod.tree, pretty = 0)
mod.tree.cv <- cv.tree(mod.tree, FUN = prune.tree)
mod.tree.cv
plot(mod.tree.cv$size, mod.tree.cv$dev, type = "b")
#Seems like best size of the tree is 6 - lets use this to prune the tree
install.packages(c("randomForest", "tree"))
install.packages(c("randomForest", "tree"))
mod.tree.prune <- prune.tree(mod.tree, best = 6)
summary(mod.tree.prune)
plot(mod.tree.prune)
text(mod.tree.prune, pretty = 0)
#Find the best size of the tree - the point where deviance(dev) is lowest
mod.tree.cv <- cv.tree(mod.tree, FUN = prune.tree)
mod.tree.cv
plot(mod.tree.cv$size, mod.tree.cv$dev, type = "b")
#Seems like best size of the tree is 6 - lets use this to prune the tree
mod.tree.prune <- prune.tree(mod.tree, best = 6)
summary(mod.tree.prune)
plot(mod.tree.prune)
text(mod.tree.prune, pretty = 0)
### Predict salary, using the pruned tree ###
hit$Salary.Pred <- predict(mod.tree.prune, newdata = hit)
#Reapply 20% classification threshold for predicted salary
hit$Salary.Pred.Class <- ifelse(hit$Salary.Pred > quantile(hit$Salary.Pred, probs = 0.8), "High", "Low")
table(hit$Salary.Pred.Class)
#For training set
table(TrueSalary = hit$SalaryClass[train], PredSalary = hit$Salary.Pred.Class[train]) #How many instances were misclassified? Error rates?
table(TrueSalary = hit$SalaryClass[test], PredSalary = hit$Salary.Pred.Class[test]) #How many instances were misclassified? Error rates?
install.packages("randomForest")
### Random Forests ###
library(randomForest)
#re-saving the original data into a different dataframe
hit2 <- Hitters
hit2 <- na.omit(hit2)
#Grow a random forest to predict salary with all other variables
mod.rf <- randomForest(Salary ~., data = hit2, importance = TRUE, ntree  = 100)
plot(mod.rf)
varImpPlot(mod.rf)
#Tuning the forest to find the best m (number of candidates randomly sampled at each split)
tuneRF(x = subset(hit2, select = -Salary), y = hit2$Salary, ntreeTry = 200, mtryStart = 1, stepFactor = 2, plot = TRUE, improve = 0.001)
#Use this m value to build the forest (on the training set) and predict on the test set
mod.rf.opt <- randomForest(Salary ~., data = hit2[train,], importance = TRUE, ntree  = 100, mtry = 4)
varImpPlot(mod.rf.opt)
rf.pred.salary <- predict(mod.rf.opt, data = hit2[test,])
tuneRF(x = subset(hit2, select = -Salary), y = hit2$Salary, ntreeTry = 200, mtryStart = 1, stepFactor = 2, plot = TRUE, improve = 0.0001)
?tuneRF
### Random Forests ###
library(randomForest)
#re-saving the original data into a different dataframe
hit2 <- Hitters
hit2 <- na.omit(hit2)
#Grow a random forest to predict salary with all other variables
mod.rf <- randomForest(Salary ~., data = hit2, importance = TRUE, ntree  = 100)
plot(mod.rf)
varImpPlot(mod.rf)
#Tuning the forest to find the best m (number of candidates randomly sampled at each split)
tuneRF(x = subset(hit2, select = -Salary), y = hit2$Salary, ntreeTry = 200, mtryStart = 1, stepFactor = 2, plot = TRUE, improve = 0.0001)
#Use this m value to build the forest (on the training set) and predict on the test set
mod.rf.opt <- randomForest(Salary ~., data = hit2[train,], importance = TRUE, ntree  = 100, mtry = 4)
varImpPlot(mod.rf.opt)
rf.pred.salary <- predict(mod.rf.opt, data = hit2[test,])
