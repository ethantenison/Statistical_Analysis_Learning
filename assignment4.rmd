---
title: "Assignment 4"
author: "Ethan Tenison"
date: "4/13/2020"
output: word_document
---




# Q1: Model Selection in the abstract (20 points)
The decennial census is a massive undertaking that involves hiring thousands of short-term employees
and, locally, volunteering opportunities. The Census’ 2016 Planning Database Block Group Data contains
344 variables describing 220,357 block groups. The variable Mail_Return_Rate_CEN_2010 is the 2010
Census Mail Return Rate – suppose that when this rate is low in a block group, more people are needed
to help with the count. We know the return rates, along with a lot of other characteristics of block
groups, for 2010 but a lot has changed in ten years. Understanding the most important drivers of low
return rates in 2010 could help with planning for 2020.
<br />



### (a) Describe how you might use forward vs. backward stepwise selection to find the best model of
low return rates. Explain the advantages and disadvantages of one approach over the other.
Explain how these approaches compare to best subset selection.
<br />



Using forward and backward stepwise selection will help us decide which features are most important in our model, especially when n < p. Both are computationally superior to best subset selection, where 2^p models are created. While FSS is computationally superior, there is still a chance that the selected model  will be the best. FSS adds predictors one at a time, and each successive step might not include the same vital predictors as the previous one. BSS does not have this problem because all predictors are added into the model first, and then substracted. The downside of BSS is that n must be greater than p. 


### (b) How would you use model-level comparison metrics to determine when one model is better
than another? Explain how a “penalty” works and how they manifest differently in different
comparison metrics.
<br />

There are essentially two methods for comparing models, make adjustments to the training error to account for overfitting, and directly estimating test error through cross-validation. 
Since R^2 and MSE increase and decrease monotonically as predictors are added, metrics such as Cp, AIC, BIC, and adjusted R^2 are used. Each of them has a penalty term which increases as does the number of predictors.
<br />

**C~p~** = \frac{1}{n}(RSS - 2d$\hat\sigma^2$), where d is the number of predictors, and $\hat\sigma^2$ is an estimate of the error associated with each response measurement. THis is done to adjust for the decline in RSS that occurs when more predictors are added. 

<br />
**AIC** = \frac{1}{n$\hat\sigma^2$}(RSS - 2d$\hat\sigma^2$), which uses maximum likelihood and is proportional to  *C~p~*

<br />
**BIC** = \frac{1}{n$\hat\sigma^2$}(RSS - log(n)d$\hat\sigma^2$), which uses a log instead of 2d, resulting in a larger penality for a high number of predictors. 


<br />
**Adjusted $R^2$ ** = \frac{\frac{RSS}{n-d-1}}{\frac{TSS}{n-1}}, the result being that adding predictors that are just noise will only slightly decrease the RSS. 


# Q2: Model Selection in data (40 points)

```{r clean, cache=TRUE}
library(readr)
library(dplyr)
library(caTools)


set.seed(27)
data <- read_csv("pdb2016bgv8_us_clean (1).csv")


data <- filter(data, data$State_name == "Texas" & data$County_name == "Travis County") 
drop_columns <- c("TEA_Update_Leave_CEN_2010", "BILQ_Mailout_count_CEN_2010", "BILQ_Frms_CEN_2010",
"pct_TEA_Update_Leave_CEN_2010", "pct_BILQ_Mailout_count_CEN_2010", "Low_Response_Score",
"Census_Mail_Returns_CEN_2010")
data <- data[ , !(names(data) %in% drop_columns)]
data_noID <- data[9:ncol(data)]
drop_LD_columns <- c("AIAN_LAND", "Tot_Population_CEN_2010", "Females_CEN_2010", "Females_ACS_10_14",
"Pop_65plus_CEN_2010", "Pop_65plus_ACS_10_14", "Non_Inst_GQ_CEN_2010", "Pop_5yrs_Over_ACS_10_14",
"Pop_25yrs_Over_ACS_10_14", "ENG_VW_ACS_10_14", "NonFamily_HHD_CEN_2010", "NonFamily_HHD_ACS_10_14",
"Tot_Prns_in_HHD_CEN_2010", "Tot_Occp_Units_CEN_2010", "Tot_Occp_Units_ACS_10_14", "Tot_Vacant_Units_CEN_2010",
"Tot_Vacant_Units_ACS_10_14", "Owner_Occp_HU_CEN_2010", "Owner_Occp_HU_ACS_10_14",
"Valid_Mailback_Count_CEN_2010", "pct_Females_CEN_2010", "pct_Pop_5yrs_Over_ACS_10_14",
"pct_Not_MrdCple_HHD_CEN_2010", "pct_Vacant_Units_ACS_10_14", "pct_Owner_Occp_HU_CEN_2010",
"pct_TEA_MailOutMailBack_CEN_2010", "pct_Deletes_CEN_2010")
data_noID_noMC <- data_noID [ , !(names(data_noID) %in% drop_LD_columns)]
data_noID_noMC <- data_noID_noMC [, -grep("pct_", colnames(data_noID_noMC))]

split = sample.split(data_noID_noMC$Mail_Return_Rate_CEN_2010, SplitRatio = 0.80)
train = subset(data_noID_noMC, split == TRUE)
test = subset(data_noID_noMC, split == FALSE)

```

### (a) Use both forward and backward stepwise selection to find the best models containing 10 or
fewer predictors. List the predictors in the order in which they enter the model for both
methods. 

```{r forward_backward}
library(caret)
library(leaps)
library(MASS)


# Fit the full model 
#full.model <- lm(Mail_Return_Rate_CEN_2010 ~., data = train)
# Stepwise regression model
#step.model <- stepAIC(full.model, direction = "forward", steps = 10, 
#                      trace = FALSE)
#summary(step.model)

#models <- regsubsets(Mail_Return_Rate_CEN_2010~., data = train, nvmax = 10, method = "forward")
#summary(models)

library(olsrr)

model<-lm(Mail_Return_Rate_CEN_2010~., data = train)

FWDfit.p<-ols_step_forward_p(model,penter=.0002)

#This gives you the short summary of the models at each step
print("Forward")
FWDfit.p



```

```{r backward}


BWDfit.p <-ols_step_backward_p(model, prem = .000001)

#This gives you the short summary of the models at each step
print("Backward")
BWDfit.p





```


(b) Fit a Lasso model to the data. Use cross-validation to select a value of ${\lambda}$ using the one-standarderror rule and plot the relationship between error and ${\lambda}$ What is the test error?
<br />

Best ${\lambda}$ = 0.01142745
Test MSE = 14.92172


```{r lasso_cross}
library(glmnet)
library(plotmo)

data_noID_noMC <- na.omit(data_noID_noMC)

grid <- 10^seq(10,-2,length=1000) #these functions automatically plot lambda, so we are going to force it to plot over a range of lambda from 10^10 to 10^-2

x <- model.matrix(Mail_Return_Rate_CEN_2010~.,data_noID_noMC)
y <-data_noID_noMC$Mail_Return_Rate_CEN_2010
train <- sample(1:nrow(x), nrow(x)*.80)
test <- (-train)
y.test <- y[test]

# Checks
dim (x[train,])
length(y[train])
length(y.test)

lasso.mod <-glmnet(x[train,], y[train],alpha=1,lambda = grid)
# glmnet() function standardizes the variables by default so that they are on the same scale.
# If alpha=0 then a ridge regression model is fit, and if alpha=1 then a lasso model is fit.

dim(coef(lasso.mod ))
summary(lasso.mod)

# par(mfrow=c(1,2))
# plot_glmnet(lasso.mod, xvar = "lambda", label = 5)
# 
# plot_glmnet(lasso.mod, xvar="dev",label=5)


#how to choose best lambda
set.seed(27)
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out, label=TRUE)
coef(cv.out)

bestlam=cv.out$lambda.min
bestlam

#test MSE associated with this value of ??
lasso.pred=predict(lasso.mod,s=bestlam,newx=x[test,])
mse= mean((lasso.pred-y.test)^2)
```

(c) Examine the coefficients at the one-standard-error value of ${\lambda}$and comment on your findings.
What are some important determinants of low census mail return rates?

```{r coefficients}
library(caret)

out=glmnet(x,y,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:161,]

lasso.coef_no0 = data.frame(as.list(lasso.coef[lasso.coef!=0]))

library(tidyr)

data_long <- gather(lasso.coef_no0, coefficient, value, "X.Intercept.":"avg_Agg_House_Value_ACSMOE_10_14", factor_key=TRUE)
data_long <- mutate(data_long, absolute_value = abs(value))

head(data_long[order(data_long$absolute_value), ])


```


(d) Based on your findings in (a-c), what can you say about where to direct resources to help
complete the 2020 Census? Who needs to know this information?