---
title: "Assignment 5, SVM & Trees"
author: "Ethan Tenison"
date: "4/25/2020"
output: word_document
---

# Q1: Support Vector Machines in the abstract (20 points)
Energy poverty is at the center of a current discussion of equity. Electric Service Payment Assistance is
one of the top reasons people call the [2-1-1 Navigation Center](https://www.unitedwayaustin.org/CNT2017/) operated by United Way for Greater
Austin. Local organizations like [TEPRI](http://www.txenergypoverty.org/) are active in working to address energy poverty in Texas. Suppose
you had access to geospatial data of the location of 2-1-1 calls in Central Texas. Some of these calls are
related to energy bill assistance, others have other topics. Your goal is to uncover communities facing
energy poverty. 


<br/>

### (a) In the context of the support vector classifier, discuss the role of margins, the tuning parameter, and the relationship between them. 
<br/>

The maximal margin classifier selects a hyperplane, out of an infinite number, that has the greatest perpendicular distance from all of the observations on the margin, called the support vectors. This is hypeyplane is called the maximimal margin hyperplane. This often leads to overfitting, as only a few number of observations determine the hyperplane. SVC use what is called a "soft margin", where some observations are allowed to be on the wrong side of the boundary. This is where the tuning parameter comes in. The tuning parameter C is used to decide how many incorrectly labeled observations are allowed within the margins of the hyperplane. The soft margin is set by ensuring the sum of the error terms is less than or equal to C, which is chosen through cross validation to find the optimal point for the variance and bias trade off. 


### (b) Suppose you use SVM with a radial kernel to classify regions by x,y coordinates. Explain the intuition behind how a radial kernel works. If, in the future, more calls related to energy bill assistance come from inside a region that you have identified as a communities facing energy poverty, how does this impact your SVM? Why? What does it mean if a call comes in that results in a change to the SVM?
<br/>

Radial kernels help us to generate non-linear decision boundaries when our data is inherently not linear. Kernels in general create nonlinear boundaries by doing what's called feature expansion, where we convert features to higher dimensions. This can be done with polynomial kernels, but as we increase the number of dimensions we have to deal with the problem of overfitting. We can solve this issue by using a radial kernel, which can have a smoother decision boundaries based on the tuning parameters. Based on the textbook, it also does better when the data is inherently circular. 


# Q2: Regression trees in data (40 points)
A portion of residential energy consumption goes towards a legitimate end use, but there is plenty of room to increase energy efficiency in homes (for example, by repairing leaky ducts). Often, utilities themselves offer rebates for their customers to increase their energy efficiency, e.g. [Austin Energy’s free
Weatherization program](https://savings.austinenergy.com/rebates/residential/offerings/home-improvements/weatherization). Targeting these resources to homes that could benefit from them can be difficult. Use the data from [Austin’s Energy Conservation Audit and Disclosure](https://data.austintexas.gov/Utilities-and-City-Services/Map-2009-2013-ECAD-Residential-Energy-Audit-Data/wqi4-2ivr) to devise a simple way to
sort homes that identifies those with high levels of duct leakage. Please consult and adapt the included SAL_A5q2 script to clean the data (and generate some nifty maps along the way). 
<br/>


```{r data_cleaning, echo=FALSE}

# A5 script
# RECS analysis file
# D. Cale Reeves
# UT Austin, Energy Systems Transformation Group
# Created: 2019/01/29
#
# 

#---------------#
# Run Parameters 
#---------------#
options("width"=180)

OutFile <- file("200315_SAL_A5q2_sol_DCR_Out.txt")
sink(OutFile, append=FALSE, type=c("output"))
sink(OutFile, append=FALSE, type=c("message"))
set.seed(1)



#---------------#
# Libraries
#---------------#

library(ggmap)
library(MASS)
library(reshape2)
library(scales)
library(tree)

#---------------#
# Load Data
#---------------#

EnergyAudit <- read.csv("Map__2009-2013__ECAD_Residential_Energy_Audit_Data.csv")
#head(EnergyAudit)
#tail(EnergyAudit)
print(sprintf("EnergyAudit data has %d rows with %d columns", nrow(EnergyAudit), ncol(EnergyAudit)))

#---------------#
# Clean Data
#---------------#

#str(EnergyAudit)
EnergyAudit$Address <- as.character(EnergyAudit$Address)

EnergyAudit$Coords <- sapply(EnergyAudit$Address, function (i) {
	#print(i)
	#print(gregexpr(pattern ='\n',i))
	#print(gregexpr(pattern ='\n',i)[[1]][2])
	#print(nchar(i))
	#print(substr(i, gregexpr(pattern ='\n',i)[[1]][2]+1, nchar(i)))
	substr(i, gregexpr(pattern ='\n',i)[[1]][2]+1, nchar(i))
})

head(EnergyAudit$Coords)

EnergyAudit$Lat <- sapply(EnergyAudit$Coords, function (i) {
	#print(i)
	#print(gregexpr(pattern ='\n',i))
	#print(gregexpr(pattern =', ',i)[[1]][1])
	#print(nchar(i))
	#print(substr(i, 2, gregexpr(pattern =', ',i)[[1]][1]-1))
	as.numeric(substr(i, 2, gregexpr(pattern =', ',i)[[1]][1]-1))
})

head(EnergyAudit$Lat)

EnergyAudit$Long <- sapply(EnergyAudit$Coords, function (i) {
	#print(i)
	#print(gregexpr(pattern ='\n',i))
	#print(gregexpr(pattern =', ',i)[[1]][1])
	#print(nchar(i))
	#print(substr(i, gregexpr(pattern =', ',i)[[1]][1]+2, nchar(i)-1))
	as.numeric(substr(i, gregexpr(pattern =', ',i)[[1]][1]+2, nchar(i)-1))
})

#head(EnergyAudit)


EnergyAudit_clean <-  EnergyAudit[EnergyAudit$Duct.System.1...Type != "",]
nrow(EnergyAudit_clean)
#head(EnergyAudit_clean)

EnergyAudit_clean <- EnergyAudit_clean[,c("TCAD.or.WCAD.Property.ID..s.", 
	"Duct.System.1...Type", 
	"Duct.System.1...RValue", 
	"Duct.System.1...Return.Sizing", 
	"Duct.System.1.....Leakage",
	"System.1...Location.Air.Handler",
	"System.1...Air.Handler.Type",
	"System.1...Age..years.",
#	"System.1...EER",
	"System.1...sqft.ton",
	"Water.Heater...Fuel.Type",
	"Water.Heating...Tank.Type",
	"Toilet.Type",
	"Programmable.Thermostat.Present",
	"Window.Screen.Area.Recommended..sqft.",
	"Furnace...Fuel.Type",
	"Attic.R.Value",
	"Recommended.Additional.R.Value",
	"Year.Built",
	"Bedrooms",
	"Conditioned..sqft.",
	"Attic..sqft.",
	"Home_Type",
	"Lat",
	"Long")]
	
#head(EnergyAudit_clean)
EnergyAudit_clean <-  EnergyAudit_clean[!is.na(EnergyAudit_clean$System.1...Age..years.),]
EnergyAudit_clean$Attic..sqft.[is.na(EnergyAudit_clean$Attic..sqft.)] = 0
EnergyAudit_clean$Attic.R.Value[is.na(EnergyAudit_clean$Attic.R.Value)] = 0
EnergyAudit_clean$Recommended.Additional.R.Value[is.na(EnergyAudit_clean$Recommended.Additional.R.Value)] = 0

# summary(EnergyAudit_clean$Duct.System.1...Type, exclude=NULL) 
# summary(EnergyAudit_clean$Duct.System.1...RValue, exclude=NULL) 
# summary(EnergyAudit_clean$Duct.System.1...Return.Sizing, exclude=NULL) 
# summary(EnergyAudit_clean$Duct.System.1.....Leakage, exclude=NULL) 
# summary(EnergyAudit_clean$System.1...Location.Air.Handler, exclude=NULL) 
# summary(EnergyAudit_clean$System.1...Air.Handler.Type, exclude=NULL) 
# summary(EnergyAudit_clean$System.1...Age..years., exclude=NULL) 
# #summary(EnergyAudit_clean$System.1...EER, exclude=NULL) 
# summary(EnergyAudit_clean$System.1...sqft.ton, exclude=NULL) 
# summary(EnergyAudit_clean$Water.Heater...Fuel.Type, exclude=NULL) 
# summary(EnergyAudit_clean$Water.Heating...Tank.Type, exclude=NULL) 
# summary(EnergyAudit_clean$Toilet.Type, exclude=NULL) 
# summary(EnergyAudit_clean$Programmable.Thermostat.Present, exclude=NULL) 
# summary(EnergyAudit_clean$Window.Screen.Area.Recommended..sqft., exclude=NULL) 
# summary(EnergyAudit_clean$Furnace...Fuel.Type, exclude=NULL) 
# summary(EnergyAudit_clean$Attic.R.Value, exclude=NULL) 
# summary(EnergyAudit_clean$Recommended.Additional.R.Value, exclude=NULL) 
# summary(EnergyAudit_clean$Year.Built, exclude=NULL) 
# summary(EnergyAudit_clean$Bedrooms, exclude=NULL) 
# summary(EnergyAudit_clean$Conditioned..sqft., exclude=NULL) 
# summary(EnergyAudit_clean$Attic..sqft., exclude=NULL) 
# summary(EnergyAudit_clean$Home_Type, exclude=NULL) 
# summary(EnergyAudit_clean$Lat, exclude=NULL) 
# summary(EnergyAudit_clean$Long, exclude=NULL) 

nrow(EnergyAudit_clean)
nrow(na.omit(EnergyAudit_clean))

EnergyAudit_clean <- na.omit(EnergyAudit_clean)



print(sprintf("EnergyAudit_clean data: Longitude Range: (%f,%f). Width: %f", min(EnergyAudit_clean$Long),max(EnergyAudit_clean$Long),min(EnergyAudit_clean$Long) - max(EnergyAudit_clean$Long)))
print(sprintf("EnergyAudit_clean data: Latitude Range: (%f,%f). Height: %f", min(EnergyAudit_clean$Lat),max(EnergyAudit_clean$Lat),min(EnergyAudit_clean$Lat) - max(EnergyAudit_clean$Lat)))


LongRange = c(-98, -97.56)
LatRange = c(30.09, 30.47)

EnergyAudit_clean <- EnergyAudit_clean[EnergyAudit_clean$Long > min(LongRange) & EnergyAudit_clean$Long < max(LongRange), ]
EnergyAudit_clean <- EnergyAudit_clean[EnergyAudit_clean$Lat > min(LatRange) & EnergyAudit_clean$Lat < max(LatRange), ]

print(sprintf("Trimming map to: Latitude Range: (%f,%f),  Latitude Range: (%f,%f)", min(LatRange),max(LatRange),min(LongRange),max(LongRange)))

print(sprintf("EnergyAudit_clean data now has %d rows with %d columns", nrow(EnergyAudit_clean), ncol(EnergyAudit_clean)))


EnergyAudit_clean$set <- sample(c("train", "test"), size=nrow(EnergyAudit_clean), replace=TRUE, prob=c(0.70,0.30))
table(EnergyAudit_clean$set)

summary(EnergyAudit_clean$Duct.System.1.....Leakage)
EnergyAudit_clean$NeedsDuctFixed <- 0
EnergyAudit_clean$NeedsDuctFixed[EnergyAudit_clean$Duct.System.1.....Leakage > quantile(EnergyAudit_clean$Duct.System.1.....Leakage, probs=c(0.8)) ] = 1
EnergyAudit_clean$NeedsDuctFixed <- as.factor(EnergyAudit_clean$NeedsDuctFixed)
table(EnergyAudit_clean$NeedsDuctFixed, EnergyAudit_clean$set)

write.csv(EnergyAudit_clean, "assignment5_q2_cleaned.csv")

```


```{r kde}
#---------------#
# Get KDE
#---------------#

N_dens <- 100

BandwidthMultiplier <- 20

LongIncrement <- (max(LongRange)-min(LongRange))/N_dens
LatIncrement <- (max(LatRange)-min(LatRange))/N_dens
# LongIncrement
# LatIncrement

EnergyAudit_cleanDensMap100 = kde2d(EnergyAudit_clean$Long, EnergyAudit_clean$Lat, h=c(BandwidthMultiplier*LongIncrement, BandwidthMultiplier*LatIncrement) , lims=c(LongRange, LatRange), n=N_dens)
NeedsDuctFixedDensMap100 = kde2d(EnergyAudit_clean$Long[EnergyAudit_clean$NeedsDuctFixed == 1], EnergyAudit_clean$Lat[EnergyAudit_clean$NeedsDuctFixed == 1], h=c(BandwidthMultiplier*LongIncrement, BandwidthMultiplier*LatIncrement) , lims=c(LongRange, LatRange), n=N_dens)
NotNeedsDuctFixedDensMap100 = kde2d(EnergyAudit_clean$Long[EnergyAudit_clean$NeedsDuctFixed == 0], EnergyAudit_clean$Lat[EnergyAudit_clean$NeedsDuctFixed == 0], h=c(BandwidthMultiplier*LongIncrement, BandwidthMultiplier*LatIncrement) , lims=c(LongRange, LatRange), n=N_dens)



print(sprintf("EnergyAudit_clean densities calculated for a %d square grid", N_dens))


#head(EmpDensMap100)
#length(EmpDensMap100)

# Now melt them to long format
EnergyAudit_cleanDensMap100.m = melt(EnergyAudit_cleanDensMap100$z, id.var=rownames(EnergyAudit_cleanDensMap100))
names(EnergyAudit_cleanDensMap100.m) = c("Long","Lat","z")

EnergyAudit_cleanDensMap100.m$adjLong <- (EnergyAudit_cleanDensMap100.m$Long * LongIncrement) + min(LongRange)
EnergyAudit_cleanDensMap100.m$adjLat<- (EnergyAudit_cleanDensMap100.m$Lat * LatIncrement) + min(LatRange)
EnergyAudit_cleanDensMap100.m$adjz <- rescale(EnergyAudit_cleanDensMap100.m$z, to = c(0,1))
EnergyAudit_cleanDensMap100.m$STDadjz <- scale(EnergyAudit_cleanDensMap100.m$z)



NeedsDuctFixedDensMap100.m = melt(NeedsDuctFixedDensMap100$z, id.var=rownames(NeedsDuctFixedDensMap100))
names(NeedsDuctFixedDensMap100.m) = c("Long","Lat","z")

NeedsDuctFixedDensMap100.m$adjLong <- (NeedsDuctFixedDensMap100.m$Long * LongIncrement) + min(LongRange)
NeedsDuctFixedDensMap100.m$adjLat<- (NeedsDuctFixedDensMap100.m$Lat * LatIncrement) + min(LatRange)
NeedsDuctFixedDensMap100.m$adjz <- rescale(NeedsDuctFixedDensMap100.m$z, to = c(0,1))
NeedsDuctFixedDensMap100.m$STDadjz <- scale(NeedsDuctFixedDensMap100.m$z)




NotNeedsDuctFixedDensMap100.m = melt(NotNeedsDuctFixedDensMap100$z, id.var=rownames(NotNeedsDuctFixedDensMap100))
names(NotNeedsDuctFixedDensMap100.m) = c("Long","Lat","z")

NotNeedsDuctFixedDensMap100.m$adjLong <- (NotNeedsDuctFixedDensMap100.m$Long * LongIncrement) + min(LongRange)
NotNeedsDuctFixedDensMap100.m$adjLat<- (NotNeedsDuctFixedDensMap100.m$Lat * LatIncrement) + min(LatRange)
NotNeedsDuctFixedDensMap100.m$adjz <- rescale(NotNeedsDuctFixedDensMap100.m$z, to = c(0,1))
NotNeedsDuctFixedDensMap100.m$STDadjz <- scale(NotNeedsDuctFixedDensMap100.m$z)

print(sprintf("EnergyAudit_clean densities standardized over %d square grid", N_dens))


#---------------#
# Make Graphics
#---------------#

register_google(key = "AIzaSyDHoo1ZyoaGYMEQo4ZVVsl03jx67xeb0kI")
AustinMap <- ggmap(get_map(location = c(lon = -97.78, lat = 30.28), zoom = 11)) 

save(AustinMap, file="AustinMap.Rdata")
load(file="AustinMap.Rdata")

BlankMap <- AustinMap
ggsave("AustinMap.png", AustinMap)


EnergyAudit_cleanDensPlot <- AustinMap +
	geom_tile(data=EnergyAudit_cleanDensMap100.m, aes(x = adjLong, y  = adjLat, z=z, fill=z, alpha=z)) +
	geom_contour(data=EnergyAudit_cleanDensMap100.m, aes(x = adjLong, y  = adjLat, z=z)) + 
 	scale_fill_gradient(low="green", high="red", name="EnergyAudit_clean\nDensity") +
  	coord_cartesian(xlim=LongRange, ylim=LatRange) +
  	guides(alpha=FALSE) + 
  	scale_x_continuous(name="Longitude") +
	scale_y_continuous(name="Latitude") +
	theme_bw()
ggsave("MapEnergyAudit_cleanDens.png", EnergyAudit_cleanDensPlot)

NeedsDuctFixedDensMap100DensPlot <- AustinMap +
	geom_tile(data=NeedsDuctFixedDensMap100.m, aes(x = adjLong, y  = adjLat, z=z, fill=z, alpha=z)) +
	geom_contour(data=NeedsDuctFixedDensMap100.m, aes(x = adjLong, y  = adjLat, z=z)) + 
 	scale_fill_gradient(low="green", high="red", name="Needs Duct Fixed\nDensity") +
  	coord_cartesian(xlim=LongRange, ylim=LatRange) +
  	guides(alpha=FALSE) + 
  	scale_x_continuous(name="Longitude") +
	scale_y_continuous(name="Latitude") +
	theme_bw()
ggsave("MapNeedsDuctFixedDensMap100Dens.png", NeedsDuctFixedDensMap100DensPlot)

NotNeedsDuctFixedDensMap100DensPlot <- AustinMap +
	geom_tile(data=NotNeedsDuctFixedDensMap100.m, aes(x = adjLong, y  = adjLat, z=z, fill=z, alpha=z)) +
	geom_contour(data=NotNeedsDuctFixedDensMap100.m, aes(x = adjLong, y  = adjLat, z=z)) + 
 	scale_fill_gradient(low="green", high="red", name="Not Needs Duct Fixed\nDensity") +
  	coord_cartesian(xlim=LongRange, ylim=LatRange) +
  	guides(alpha=FALSE) + 
  	scale_x_continuous(name="Longitude") +
	scale_y_continuous(name="Latitude") +
	theme_bw()
ggsave("MapNotNeedsDuctFixedDensMap100Dens.png", NotNeedsDuctFixedDensMap100DensPlot)

#---------------#
# Do Decision Trees
#---------------#







```

### (a) Grow a decision tree to predict System 1 Duct Leakage in the training data on the basis of the sort of simple questions you could ask someone over the phone, e.g. “Where is your system?”;“Is your furnace gas or electric?”; “What sort of home are you in?” Grow a very large tree and print it (hint: add mindev = 0.001 and minsize = 5, you may need to adjust mindev).
<br/>

```{r decision_tree}
library(janitor)
library(dplyr)
library(readr)
df <- read.csv("assignment5_q2_cleaned.csv", stringsAsFactors = TRUE)
df <- clean_names(df)
df <- na.omit(df)
df$leakageClass <- ifelse(df$duct_system_1_leakage > quantile(df$duct_system_1_leakage, probs = 0.8), "High", "Low")
### Split data into traiing/test index ###
set.seed(1234)
train <- sample(c(TRUE,FALSE), nrow(df), rep=TRUE, prob = c(0.6,0.4))
test <- (!train)
df_train <- df[train,]


### Growing a decision tree ###

library(tree)

mod.tree <- tree(duct_system_1_leakage ~ system_1_location_air_handler + system_1_age_years + furnace_fuel_type + home_type + system_1_sqft_ton +  conditioned_sqft, data = df_train, mindev = 0.001, minsize = 10 )

#duct_system_1_type + took this out because it's hard to see XD

summary(mod.tree)

plot(mod.tree)
text(mod.tree, pretty = 0)
```




### (b) Use cross validation to prune the tree. What is the best size? Print the pruned tree. Interpret the pruned tree. How would you use this to inform targeting weatherization resources to those that need them?
<br/>


```{r tree_pruning}
### Prune tree with CV ###

#Find the best size of the tree - the point where deviance(dev) is lowest
mod.tree.cv <- cv.tree(mod.tree, FUN = prune.tree)
mod.tree.cv

plot(mod.tree.cv$size, mod.tree.cv$dev, type = "b")
#Seems like best size of the tree is 3 - lets use this to prune the tree

mod.tree.prune <- prune.tree(mod.tree, best = 3)

summary(mod.tree.prune)

plot(mod.tree.prune)
text(mod.tree.prune, pretty = 0)


```


### (c) Suppose the top 20% of leakage need immediate attention. Predict leakage using the pruned tree and apply the classification threshold. Print the confusion matrix for the test data. Calculate and discuss the error types, rates, and consequences in the context of this problem.### 
<br/>


```{r leakage_threshold}

### Predict salary, using the pruned tree ###
df$leakage.Pred <- predict(mod.tree.prune, newdata = df)
#Reapply 20% classification threshold for predicted salary
df$leakage.Pred.Class <- ifelse(df$leakage.Pred > quantile(df$leakage.Pred, probs = 0.8), "High", "Low")
table(df$leakage.Pred.Class)

### Confusion matrix ###

#For training set
print("Train set")
table(TrueLeakage = df$leakageClass[train], Predleakage = df$leakage.Pred.Class[train]) #How many instances were misclassified? Error rates?
#For test set
print("Test set")
table(TrueLeakage = df$leakageClass[test], Predleakage = df$leakage.Pred.Class[test]) #How many instances were misclassified? Error rates?


```

### (d) Comment on the “interestingness” of your findings. Identify the audience for
these findings. Craft a concise message to convey these findings in as interesting a way as you can. 
<br/>

Train: true positive rate of .09%, 

# Q3: Unsupervised learning in simulated data (40 points)
Often patterns in data manifest as self-similar groups of observations, but the labels of the groups and
the actual group membership – in a sense, the reason that the groups are self-similar – is elusive. Use
the SAL_A5q3 data generation script to generate 100 observations with 4 groups and 2 features; this is
the n100g4p2 dataset. 

### (a) Plot the data x=X1, y=X2, and color=truecluster. Comment on any true cluster that you think may be easier or more difficult to identify. Comment on any specific observations that are likely to be misidentified.
<br/>

### (b) Perform kmeans clustering for k=2:7. Plot your results with x=X1, y=X2, and color=cluster; display them in a single figure with 6 panes (hint: use grid.arrange() in the gridExtra package). Comment on the relationship between the clusters identified and what you know to be the true group membership. Without knowing the true group membership, which values of k do you prefer and why? Which values of k do you not prefer and why?
<br/>

### (c) Perform hierarchical clustering with complete linkage; print the dendrogram. Cut the tree to obtain cluster assignments for 2-7 clusters. Plot your results in a single figure with 6 panes.Comment on these clusters with respect to kmeans clustering and the true group membership. Which cut levels do you prefer and why? Which cut levels do you not prefer and why?
<br/>

### (d) Perform hierarchical clustering with average and single linkage; print the dendrograms, cut the trees to obtain cluster assignments for 2-7 clusters, and plot your results in a single figure with 6 panes. Comment on the various clustering methods you’ve used; which method(s) do you prefer and why? Which method(s) do you not prefer and why?
<br/>
