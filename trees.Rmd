---
title: "Trees"
author: "Vivek"
date: "4/10/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Clear environment
rm(list = ls())

library(tidyverse)
library(ISLR)

hit <- Hitters
hit <- na.omit(hit)

#Question: Lets continue working with the hitters dataset. Classify top 20% of the salaries as High and the rest as Low.  Select 5 variables from the  dataset that you believe will best help predict players' salary. Split the data into 60% training and 40% tet sets. Use these variables to grow a decision tree using the training set. Use cross-validation to prune the tree and find the best size. Finally, use the pruned tree to predict the salaries. Reapply the top20% salary classification for the predicted data, and print a confusion matrix for the training and test sets separately.

```

```{r}

### Apply classification threshold ###

#Compare if salary is greater than the 80th quantile (ie, salary is in the top 20% of all salaries)
hit$SalaryClass <- ifelse(hit$Salary > quantile(hit$Salary, probs = 0.8), "High", "Low")
table(hit$SalaryClass) #Note that 20% of salaries have been classified as High, like we wanted



### Select 5 potential predictors ###

names(hit)
#Lets us say we use best subset selection to identify top 5 variables (mainly since I'm clueless about baseball)
##NOTE: FOR YOUR ASSIGNMENT,NO NEED TO DO SUBSET SELECTION. JUST SELECT APPROPRIATE VARIABLES BASED ON YOUR INTUITION.
library(leaps)
coef(regsubsets(Salary ~.-SalaryClass, hit, nvmax = 5),5)
#So we've identified AtBat, Hits, CRBI, Division, PutOuts - lets use these to grow our tree


### Split data into traiing/test index ###
set.seed(1234)
train <- sample(c(TRUE,FALSE), nrow(hit), rep=TRUE, prob = c(0.6,0.4))
test <- (!train)

### Growing a decision tree ###

library(tree)
mod.tree <- tree(Salary ~ AtBat + Hits + CRBI + Division + PutOuts, data = hit[train,]) #For your assignment, Here you may also need to specify mindev = 0.001and minsize = 5

summary(mod.tree)

plot(mod.tree)
text(mod.tree, pretty = 0)



### Prune tree with CV ###

#Find the best size of the tree - the point where deviance(dev) is lowest
mod.tree.cv <- cv.tree(mod.tree, FUN = prune.tree)
mod.tree.cv

plot(mod.tree.cv$size, mod.tree.cv$dev, type = "b")
#Seems like best size of the tree is 6 - lets use this to prune the tree

mod.tree.prune <- prune.tree(mod.tree, best = 6)

summary(mod.tree.prune)

plot(mod.tree.prune)
text(mod.tree.prune, pretty = 0)


### Predict salary, using the pruned tree ###
hit$Salary.Pred <- predict(mod.tree.prune, newdata = hit)
#Reapply 20% classification threshold for predicted salary
hit$Salary.Pred.Class <- ifelse(hit$Salary.Pred > quantile(hit$Salary.Pred, probs = 0.8), "High", "Low")
table(hit$Salary.Pred.Class)


### Confusion matrix ###

#For training set
table(TrueSalary = hit$SalaryClass[train], PredSalary = hit$Salary.Pred.Class[train]) #How many instances were misclassified? Error rates?
#For test set
table(TrueSalary = hit$SalaryClass[test], PredSalary = hit$Salary.Pred.Class[test]) #How many instances were misclassified? Error rates?

```


```{r}

### Random Forests ###

library(randomForest)

#re-saving the original data into a different dataframe
hit2 <- Hitters
hit2 <- na.omit(hit2)

#Grow a random forest to predict salary with all other variables
mod.rf <- randomForest(Salary ~., data = hit2, importance = TRUE, ntree  = 100)
plot(mod.rf)
varImpPlot(mod.rf)

#Tuning the forest to find the best m (number of candidates randomly sampled at each split)
tuneRF(x = subset(hit2, select = -Salary), y = hit2$Salary, ntreeTry = 200, mtryStart = 1, stepFactor = 2, plot = TRUE, improve = 0.001)

#Use this m value to build the forest (on the training set) and predict on the test set
mod.rf.opt <- randomForest(Salary ~., data = hit2[train,], importance = TRUE, ntree  = 100, mtry = 4)
varImpPlot(mod.rf.opt)
rf.pred.salary <- predict(mod.rf.opt, data = hit2[test,])

```

