ep.star <- sample(s,size=length(reg$residuals),replace=TRUE)
# Make bootstrap Y
y.star <- fitted(reg)+ep.star
# Do bootstrap regression
x <- model.frame(reg)[,2]
bs.reg <- lm(y.star~x)
# Create bootstrapped adjusted residuals
bs.lev <- influence(bs.reg)$hat
bs.s   <- residuals(bs.reg)/sqrt(1-bs.lev)
bs.s   <- bs.s - mean(bs.s)
# Calculate draw on prediction error
xb.xb <- coef(my.reg)["(Intercept)"] - coef(bs.reg)["(Intercept)"]
xb.xb <- xb.xb + (coef(my.reg)["x"] - coef(bs.reg)["x"])*x_Np1
return(unname(xb.xb + sample(bs.s,size=1)))
}
# Do bootstrap with 1000 replications
ep.draws <- replicate(n=1000,the.replication(reg=my.reg,s=my.s.resid,x_Np1=54.2857143))
# Create prediction interval
y.p+quantile(ep.draws,probs=c(0.05,0.95))
# prediction interval using normal assumption
predict(my.reg,newdata=data.frame(x=54.2857143),interval="prediction",level=0.90)
library(dplyr)
library(janitor)
library(reshape2)
df <- read.csv("Austin_Water_-_Residential_Water_Consumption.csv")
df <- clean_names(df)
df$year <- substr(df$year_month, 1, 4)
df$year <- as.numeric(df$year)
df$month <- substr(df$year_month, 5, 6)
df$month <- as.factor(df$month)
df <- dplyr::select(df, -c(year_month))
df$customer_class <- gsub(" - ", "_", df$customer_class)
df <- filter(df, postal_code != "")
df$total_gallons[is.na(df$total_gallons)] <- 0
dfwide <- dcast(data=df, postal_code+month+year~customer_class, value.var= 'total_gallons')
dfwide <- clean_names(dfwide)
df$total_gallons[is.na(df$total_gallons)] <- 0
dfwide[is.na(dfwide)] <- 0
library(ggplot2)
options(scipen=999)
print(summary(dfwide))
total_water <- df %>% group_by(year, customer_class) %>%
summarize(total_gallons = sum(total_gallons))
tw <- ggplot(total_water, aes(x=year, y=total_gallons, group=customer_class, color=customer_class)) +
geom_line()
print(tw)
library(caTools)
logdf <- dfwide
for (i in 1:length(logdf$irrigation_residential)){
if(logdf$irrigation_residential[i] > mean(logdf$irrigation_residential)){
logdf$HiResIrr[i] <- 1
}
else if(logdf$irrigation_residential[i] <= mean(logdf$irrigation_residential)){
logdf$HiResIrr[i] <- 0
}
}
set.seed(88)
split = sample.split(logdf$HiResIrr, SplitRatio = 0.75)
logdf <- dplyr::select(logdf, -c(irrigation_residential, postal_code))
train = subset(logdf, split == TRUE)
test = subset(logdf, split == FALSE)
model <- glm(HiResIrr ~.,family=binomial(link='logit'),data=train)
summary(model)
library(ggplot2)
set.seed(666)
x <- rnorm(100, 75, 20)
x2 <- x*x
e <- rnorm(100, 0, 7)
y <- 125 + -3.8*x + 0.035*x**2
df <- data.frame(y, x, x2)
ggplot(data = df, aes(x=x, y=y)) + geom_point() + geom_vline(xintercept = 54.2857143, linetype = "dashed", color = "red")
library(boot)
model <- lm(y ~ x + x2, data = df)
summary(model)
library(boot)
bs <- function(data, indices) {
d <- data[indices,] # allows boot to select sample
fit <- lm(y ~ x + x2, data=d)
return(coef(fit))
return(-coef(fit)["x"]/(2*coef(fit)["x2"]))
}
results <- boot(data=mtcars, statistic=bs,
R=1000)
library(boot)
bs <- function(data, indices) {
d <- data[indices,] # allows boot to select sample
fit <- lm(y ~ x + x2, data=d)
return(coef(fit))
return(-coef(fit)["x"]/(2*coef(fit)["x2"]))
}
results <- boot(data=mtcars, statistic=bs,
R=1000)
results
library(boot)
bs <- function(data, indices) {
d <- data[indices,] # allows boot to select sample
fit <- lm(y ~ x + x2, data=d)
return(-coef(fit)["x"]/(2*coef(fit)["x2"]))
}
results <- boot(data=mtcars, statistic=bs,
R=1000)
results
library(boot)
bs <- function(data, indices) {
d <- data[indices,] # allows boot to select sample
fit <- lm(y ~ x + x2, data=d)
return(-coef(fit)["x"]/(2*coef(fit)["x2"]))
}
results <- boot(data=mtcars, statistic=bs,
R=10)
results
library(boot)
bs <- function(data, indices) {
d <- data[indices,] # allows boot to select sample
fit <- lm(y ~ x + x2, data=d)
return(-coef(fit)["x"]/(2*coef(fit)["x2"]))
}
results <- boot(data=mtcars, statistic=bs,
R=1000)
results
library(boot)
bs <- function(data, indices) {
d <- data[indices,] # allows boot to select sample
fit <- lm(y ~ x + x2, data=d)
return(-coef(fit)["x"]/(2*coef(fit)["x2"]))
}
results <- boot(data=df, statistic=bs,
R=1000)
results
?boot
library(ggplot2)
set.seed(666)
x <- rnorm(100, 75, 20)
x2 <- x*x
e <- rnorm(100, 0, 7)
y <- 125 + -3.8*x + 0.035*x**2
df <- data.frame(y, x, x2)
ggplot(data = df, aes(x=x, y=y)) + geom_point() + geom_vline(xintercept = 54.2857143, linetype = "dashed", color = "red")
library(boot)
model <- lm(y ~ x + x2, data = df)
summary(model)
library(boot)
bs <- function(data, indices) {
d <- data[indices,] # allows boot to select sample
fit <- lm(y ~ x + x2, data=d)
return(-coef(fit)["x"]/(2*coef(fit)["x2"]))
}
results <- boot(data=df, statistic=bs,
R=1000)
results
plot(results, index=2)
plot(results, index=x)
library(boot)
bs <- function(data, indices) {
d <- data[indices,] # allows boot to select sample
fit <- lm(y ~ x + x2, data=d)
return(-coef(fit)["x"]/(2*coef(fit)["x2"]))
}
results <- boot(data=df, statistic=bs(df, 1:100),
R=1000)
library(boot)
bs <- function(data, indices) {
d <- data[indices,] # allows boot to select sample
fit <- lm(y ~ x + x2, data=d)
return(-coef(fit)["x"]/(2*coef(fit)["x2"]))
}
results <- boot(data=df, statistic=bs,
R=1000)
results
bs(data=df, 1:100)
View(results)
View(df)
results <- boot(data=df, statistic=bs,
R=1000)
results
library(boot)
bs <- function(data, indices) {
d <- data[indices,] # allows boot to select sample
fit <- lm(y ~ x + x2, data=d)
return(-coef(fit)["x"]/(2*coef(fit)["x2"]))
}
results <- boot(data=df, statistic=bs,
R=1000)
results
library(boot)
model <- lm(y ~ x + x2, data = df)
summary(model)
library(boot)
model <- lm(y ~ x + x2, data = df)
summary(model)
newdata <- data.frame(x=54.2857143)
predict(model, newdata,interval="prediction")
library(boot)
model <- lm(y ~ x + x2, data = df)
summary(model)
library(boot)
bs <- function(data, indices) {
d <- data[indices,] # allows boot to select sample
fit <- lm(y ~ x + x2, data=d, subset= indices)
return(-coef(fit)["x"]/(2*coef(fit)["x2"]))
}
results <- boot(data=df, statistic=bs,
R=1000)
library(ggplot2)
set.seed(666)
x <- rnorm(100, 75, 20)
x2 <- x*x
e <- rnorm(100, 0, 7)
y <- 125 + -3.8*x + 0.035*x**2
df <- data.frame(y, x, x2)
ggplot(data = df, aes(x=x, y=y)) + geom_point() + geom_vline(xintercept = 54.2857143, linetype = "dashed", color = "red")
library(boot)
model <- lm(y ~ x + x2, data = df)
summary(model)
library(boot)
bs <- function(data, indices) {
d <- data[indices,] # allows boot to select sample
fit <- lm(y ~ x + x2, data=d, subset= indices)
return(-coef(fit)["x"]/(2*coef(fit)["x2"]))
}
results <- boot(data=df, statistic=bs,
R=1000)
results
set.seed(2)
rand <- sample(nrow(atx))
k1row <- rand[rand %% 5 + 1 == 1]
k2row <- rand[rand %% 5 + 1 == 2]
k3row <- rand[rand %% 5 + 1 == 3]
k4row <- rand[rand %% 5 + 1 == 4]
k5row <- rand[rand %% 5 + 1 == 5]
k1fold <- atx[k1row,]
k2fold <- atx[k2row,]
k3fold <- atx[k3row,]
k4fold <- atx[k4row,]
k5fold <- atx[k5row,]
print("Summary Statistics for 5 folds")
summary(k1fold$hi_use)
summary(k2fold$hi_use)
summary(k3fold$hi_use)
summary(k4fold$hi_use)
summary(k5fold$hi_use)
#model with k1fold as test
model <- glm(hi_use~ customer_class +median_home_value + median_household_income + median_rent ,family=binomial(link='logit'),data=rbind(k2fold, k3fold,k4fold,k5fold))
p <- predict(model, k1fold, type = "response")
p_class <- ifelse(p > 0.5, "Predict 1", "Predict 0")
print(table(p_class, k1fold[["hi_use"]]))
#model with k2fold as test
model <- glm(hi_use~ customer_class +median_home_value + median_household_income + median_rent ,family=binomial(link='logit'),data=rbind(k1fold, k3fold,k4fold,k5fold))
p <- predict(model, k2fold, type = "response")
p_class <- ifelse(p > 0.5, "Predict 1", "Predict 0")
print(table(p_class, k2fold[["hi_use"]]))
#Model with k3fold as test
model <- glm(hi_use~ customer_class +median_home_value + median_household_income + median_rent ,family=binomial(link='logit'),data=rbind(k2fold, k1fold,k4fold,k5fold))
p <- predict(model, k3fold, type = "response")
p_class <- ifelse(p > 0.5, "Predict 1", "Predict 0")
print(table(p_class, k3fold[["hi_use"]]))
#Model with k4fold as test
model <- glm(hi_use~ customer_class +median_home_value + median_household_income + median_rent ,family=binomial(link='logit'),data=rbind(k2fold, k1fold,k3fold,k5fold))
p <- predict(model, k4fold, type = "response")
p_class <- ifelse(p > 0.5, "Predict 1", "Predict 0")
print(table(p_class, k4fold[["hi_use"]]))
#Model with k5fold as test
model <- glm(hi_use~ customer_class +median_home_value + median_household_income + median_rent ,family=binomial(link='logit'),data=rbind(k2fold, k1fold,k3fold,k4fold))
p <- predict(model, k5fold, type = "response")
p_class <- ifelse(p > 0.5, "Predict 1", "Predict 0")
print(table(p_class, k5fold[["hi_use"]]))
install.packages(c("ISLR", "leaps"))
knitr::opts_chunk$set(echo = TRUE)
# Clear environment
rm(list = ls())
library(ISLR) #Has all the datasets used in the textbook
hit <- Hitters #Save Hitters dataset
# Hitters is the MLB data from 1986, 1987 seasons - check details in help section ISLR>>Hitters
# Refer to ISLR section 6.5
sum(is.na(hit)) #Check for NAs
hit <- na.omit(hit) #Remove rows with NAs to retain complete rows
#install.packages("leaps") #subset selection functions are in leaps package
library(leaps)
# Subset selection can be used to select more important predictors
# Three types - best subset selection, forward selection and backward selection
# For the hitters dataset, predict salary based on player stats
# Full model
mod.full <- regsubsets(Salary ~ ., data = hit, nvmax = 19) #If you want smaller number of subsets, change nvmax
# Note the Salary ~. notation - means include all other predictors in the dataframe
mod.full.sum <- summary(mod.full)
mod.full.sum
mod.full.sum$rsq #notice that Rsqr always increases
mod.full.sum$adjr2 #Adj Rsqr and bic first increases, but decreases as you add more and more predictors
mod.full.sum$bic
plot(mod.full, scale = "bic") # Top row suggests variables for best models ranked by bic, note the 6 suggested variables for the model with best bic
which.min(mod.full.sum$bic) # Can also identify the model with minimum bic (or max adj Rsq or any other criteria) directly
coef(mod.full, 6) #display the coefficient for these 6 variables
# Forward stepsise selection
mod.fwd <- regsubsets(Salary ~ ., data = hit, nvmax = 8, method = "forward") #Models with upto 8 predictors
mod.fwd.sum <- summary(mod.fwd)
mod.fwd.sum #Notice which variables are selected in 1,2,3....19 variable models respectively and in what order
# Backward stepwise selection
mod.bck <- regsubsets(Salary ~ ., data = hit, nvmax = 8, method = "backward")#Models with upto 8 predictors
mod.bck.sum <- summary(mod.bck)
mod.bck.sum #Notice which variables are selected in 19,18,17...1 variable models respectively and in what order
plot(mod.fwd, scale = "bic") #Notice the variables identified in the best model
plot(mod.bck, scale = "bic") #Notice the variables identified in the best model - are they the same as fwd selection?
View(hit)
mod.full <- regsubsets(Salary ~ ., data = hit, nvmax = 19) #If you want smaller number of subsets, change nvmax
# Note the Salary ~. notation - means include all other predictors in the dataframe
mod.full.sum <- summary(mod.full)
mod.mod.full
mod.full.sum
mod.full.sum$rsq #notice that Rsqr always increases
mod.full.sum$adjr2 #Adj Rsqr and bic first increases, but decreases as you add more and more predictors
mod.full.sum$bic
plot(mod.full, scale = "bic")
plot(mod.full, scale = "bic")
which.min(mod.full.sum$bic)
coef(mod.full, 6)
mod.fwd <- regsubsets(Salary ~ ., data = hit, nvmax = 8, method = "forward") #Models with upto 8 predictors
mod.fwd.sum <- summary(mod.fwd)
mod.fwd.sum
mod.bck <- regsubsets(Salary ~ ., data = hit, nvmax = 8, method = "backward")#Models with upto 8 predictors
mod.bck.sum <- summary(mod.bck)
mod.bck.su
mod.bck <- regsubsets(Salary ~ ., data = hit, nvmax = 8, method = "backward")#Models with upto 8 predictors
mod.bck.sum <- summary(mod.bck)
mod.bck.sum
plot(mod.fwd, scale = "bic") #Notice the variables identified in the best model
plot(mod.bck, scale = "bic")
install.packages("glmnet")
# Clear environment
rm(list = ls())
library(ISLR) #Has all the datasets used in the textbook
hit <- Hitters #Save Hitters dataset
# Hitters is the MLB data from 1986, 1987 seasons - check details in help section ISLR>>Hitters
# Refer to ISLR section 6.6
sum(is.na(hit)) #Check for NAs
hit <- na.omit(hit) #Remove rows with NAs to retain complete rows
#install.packages("glmnet")
library(glmnet) #Ridge and Lasso regressions are in glmnet pa
x <- model.matrix(Salary ~., hit)[,-1] #mo
View(x)
y <- hit$Salary
knitr::opts_chunk$set(echo = TRUE)
# Create training and test index
set.seed (1)
train <- sample(c(TRUE,FALSE), nrow(hit), rep=TRUE, prob = c(0.8,0.2))
test <- (!train)
mod.ridge <- glmnet(x[train,],y[train],alpha = 0) #for ridge regression use alpha = 0
plot(mod.ridge, xvar = "lambda")
cv.ridge <- cv.glmnet(x[train,], y[train], alpha = 0) #default number of folds is 10
names(cv.ridge) # Note what variables are stored in the CV output
plot(cv.ridge) # Plot MSE vs Lambda
library(reshape2)
library(readr)
library(janitor)
library(dplyr)
water <- read_csv("Austin_Water_-_Residential_Water_Consumption.csv")
water <- clean_names(water)
water$customer_class <- gsub(" - ", "_", water$customer_class)
water$customer_class <- gsub("-", "_", water$customer_class)
water$year <- as.numeric(substr(water$year_month, 1, 4))
water$month <- as.numeric(substr(water$year_month, 5, 6))
water <- water[water$postal_code != "", ]
water <- na.omit(water)
water <- as.data.frame(aggregate(data=water, total_gallons~customer_class+year+postal_code, sum))
water <- water[water$year == 2014,]
water$hi_use <- 0
water$hi_use[water$total_gallons > mean(water$total_gallons)] = 1
water$zip_code <- water$postal_code
houses <- read_csv("2014_Housing_Market_Analysis_Data_by_Zip_Code.csv")
houses <- clean_names(houses)
atx <- inner_join(water, houses, by="zip_code")
atx$hi_use <- as.factor(atx$hi_use)
library(caTools)
set.seed(555)
split = sample.split(atx$hi_use, SplitRatio = 0.80)
train = subset(atx, split == TRUE)
test = subset(atx, split == FALSE)
print("Train")
print(summary(train$hi_use))
print("Test")
print(summary(test$hi_use))
model <- glm(hi_use~ customer_class +median_home_value + median_household_income + median_rent ,family=binomial(link='logit'),data=train)
summary(model)
library(caret)
p <- predict(model, test, type = "response")
print(summary(p))
p_class <- ifelse(p > 0.5, "Predict 1", "Predict 0")
print(table(p_class, test[["hi_use"]]))
set.seed(2)
split = sample.split(atx$hi_use, SplitRatio = 0.80)
train = subset(atx, split == TRUE)
test = subset(atx, split == FALSE)
print(summary(train$hi_use))
print(summary(test$hi_use))
model <- glm(hi_use~ customer_class +median_home_value + median_household_income + median_rent ,family=binomial(link='logit'),data=train)
summary(model)
p <- predict(model, test, type = "response")
print(summary(p))
p_class <- ifelse(p > 0.5, "Predict 1", "Predict 0")
print(table(p_class, test[["hi_use"]]))
set.seed(2)
rand <- sample(nrow(atx))
k1row <- rand[rand %% 5 + 1 == 1]
k2row <- rand[rand %% 5 + 1 == 2]
k3row <- rand[rand %% 5 + 1 == 3]
k4row <- rand[rand %% 5 + 1 == 4]
k5row <- rand[rand %% 5 + 1 == 5]
k1fold <- atx[k1row,]
k2fold <- atx[k2row,]
k3fold <- atx[k3row,]
k4fold <- atx[k4row,]
k5fold <- atx[k5row,]
print("Summary Statistics for 5 folds")
summary(k1fold$hi_use)
summary(k2fold$hi_use)
summary(k3fold$hi_use)
summary(k4fold$hi_use)
summary(k5fold$hi_use)
#model with k1fold as test
model <- glm(hi_use~ customer_class +median_home_value + median_household_income + median_rent ,family=binomial(link='logit'),data=rbind(k2fold, k3fold,k4fold,k5fold))
p <- predict(model, k1fold, type = "response")
p_class <- ifelse(p > 0.5, "Predict 1", "Predict 0")
print(table(p_class, k1fold[["hi_use"]]))
#model with k2fold as test
model <- glm(hi_use~ customer_class +median_home_value + median_household_income + median_rent ,family=binomial(link='logit'),data=rbind(k1fold, k3fold,k4fold,k5fold))
p <- predict(model, k2fold, type = "response")
p_class <- ifelse(p > 0.5, "Predict 1", "Predict 0")
print(table(p_class, k2fold[["hi_use"]]))
#Model with k3fold as test
model <- glm(hi_use~ customer_class +median_home_value + median_household_income + median_rent ,family=binomial(link='logit'),data=rbind(k2fold, k1fold,k4fold,k5fold))
p <- predict(model, k3fold, type = "response")
p_class <- ifelse(p > 0.5, "Predict 1", "Predict 0")
print(table(p_class, k3fold[["hi_use"]]))
#Model with k4fold as test
model <- glm(hi_use~ customer_class +median_home_value + median_household_income + median_rent ,family=binomial(link='logit'),data=rbind(k2fold, k1fold,k3fold,k5fold))
p <- predict(model, k4fold, type = "response")
p_class <- ifelse(p > 0.5, "Predict 1", "Predict 0")
print(table(p_class, k4fold[["hi_use"]]))
#Model with k5fold as test
model <- glm(hi_use~ customer_class +median_home_value + median_household_income + median_rent ,family=binomial(link='logit'),data=rbind(k2fold, k1fold,k3fold,k4fold))
p <- predict(model, k5fold, type = "response")
p_class <- ifelse(p > 0.5, "Predict 1", "Predict 0")
print(table(p_class, k5fold[["hi_use"]]))
library(readr)
pdb2016bgv8_us_clean <- read_csv("pdb2016bgv8_us_clean.csv")
View(pdb2016bgv8_us_clean)
install.packages(c("fastDummies", "ggfortify", "pls"))
knitr::opts_chunk$set(echo = TRUE)
# Clear environment
rm(list = ls())
library(ISLR) #Has all the datasets used in the textbook
hit <- Hitters #Save Hitters dataset
# Hitters is the MLB data from 1986, 1987 seasons - check details in help section ISLR>>Hitters
sum(is.na(hit)) #Check for NAs
hit <- na.omit(hit) #Remove rows with NAs to retain complete rows
str(hit) # Note that 3 predictors are in factor format. PCA/PCR analyses accept only numerical inputs. So we need to convert the factors into dummy variables
#install.packages("fastDummies")
library(fastDummies)
install.packages("fastDummies")
## PCR ##
#Find the best PCR and PLS models to predic Salary using other variables in the hitters dataset
# Create training and test index
set.seed (1)
train <- sample(c(TRUE,FALSE), nrow(hit), rep=TRUE, prob = c(0.8,0.2))
test <- (!train)
#install.packages("pls")
library(pls)
install.packages("pls")
## PCR ##
#Find the best PCR and PLS models to predic Salary using other variables in the hitters dataset
# Create training and test index
set.seed (1)
train <- sample(c(TRUE,FALSE), nrow(hit), rep=TRUE, prob = c(0.8,0.2))
test <- (!train)
#install.packages("pls")
library(pls)
#install.packages("pls")
install.packages("pls")
## PCR ##
#Find the best PCR and PLS models to predic Salary using other variables in the hitters dataset
# Create training and test index
set.seed (1)
train <- sample(c(TRUE,FALSE), nrow(hit), rep=TRUE, prob = c(0.8,0.2))
test <- (!train)
#install.packages("pls")
library(pls)
mod.pcr <- pcr(Salary ~., data = hit[train,], scale = TRUE, center = TRUE, validation = "CV")
summary(mod.pcr) # Notice how many PCs do you need to explain 60% of variation in salary? [~16]
validationplot(mod.pcr, val.type = "MSEP", main = "CV Plot for PCR") # How many PCs would you choose based on the CV plot?
pred.pcr <- predict(mod.pcr, hit[test,], ncomp = 2) #predict using test set
mean((pred.pcr - hit$Salary[test])^2) #calculate test MSE
## PLS ##
mod.plsr <- plsr(Salary ~., data = hit[train,], scale = TRUE, center = TRUE, validation = "CV")
summary(mod.plsr) # Notice how many PCs do you need to explain 60% of variation in salary? [~7]
validationplot(mod.plsr, val.type = "MSEP", main = "CV Plot for PLS") # How many PCs would you choose based on the CV plot?
pred.plsr <- predict(mod.plsr, hit[test,], ncomp = 2) #predict using test set
mean((pred.plsr - hit$Salary[test])^2) #calculate test MSE
pred.plsr <- predict(mod.plsr, hit[test,], ncomp = 2) #predict using test set
mean((pred.plsr - hit$Salary[test])^2) #calculate test MSE
?sample.split
library(caTools)
?sample.split
library(readr)
pdb2016bgv8_us_clean <- read_csv("pdb2016bgv8_us_clean.csv")
View(pdb2016bgv8_us_clean)
colnames(pdb2016bgv8_us_clean)
library(readr)
data <- read_csv("pdb2016bgv8_us_clean.csv")
library(caTools)
set.seed(555)
split = sample.split(data$Mail_Return_Rate_CEN_2010, SplitRatio = 0.80)
train = subset(data, split == TRUE)
test = subset(data, split == FALSE)
